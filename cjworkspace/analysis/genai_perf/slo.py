#!/usr/bin/env python3
"""
æŒç»­æ€§èƒ½æµ‹è¯•è„šæœ¬ - åŸºäºdistserve_benchmark
æ”¯æŒåœ¨ç‰¹å®šå¹¶å‘æ¡ä»¶ä¸‹æŒç»­æµ‹è¯•60ç§’ï¼Œå¹¶æ¯”è¾ƒå¤šä¸ªéƒ¨ç½²çš„æ€§èƒ½
"""

import json
import subprocess
import time
import os
import sys
import threading
import concurrent.futures
from typing import List, Dict, Tuple, Optional
import statistics
from pathlib import Path
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
script_dir = Path(__file__).parent
project_root = script_dir.parent.parent
sys.path.append(str(project_root / "benchmarks" / "utils"))
from genai import run_genai_perf

class ContinuousBenchmark:
    """æŒç»­æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self, model_name: str = None):
        self.model_name = model_name or os.environ.get('DEPLOYMENT_MODEL_ID', '/shared-models/DeepSeek/DeepSeek-R1-Distill-Qwen-7B')
        print(f"model_name: {self.model_name}")
        self.results = {}
        self.start_time = None
        
        # SLOé…ç½®
        self.slo_configs = {
            'ultra_strict': {'ttft': 4000, 'tpot': 20},
            'strict': {'ttft': 4000, 'tpot': 40},
            'moderate': {'ttft': 15000, 'tpot': 20},
            'loose': {'ttft': 15000, 'tpot': 40},
        }
        
    def run_continuous_test(self, 
                          deployment_name: str, 
                          service_url: str, 
                          concurrency: int, 
                          duration_seconds: int = 60,
                          slo_config: str = 'moderate') -> Dict:
        """
        è¿è¡ŒæŒç»­æµ‹è¯•
        
        Args:
            deployment_name: éƒ¨ç½²åç§°
            service_url: æœåŠ¡URL
            concurrency: å¹¶å‘æ•°
            duration_seconds: æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            slo_config: SLOé…ç½®åç§°
        """
        
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯•éƒ¨ç½²: {deployment_name}")
        print(f"   æœåŠ¡URL: {service_url}")
        print(f"   å¹¶å‘æ•°: {concurrency}")
        print(f"   æŒç»­æ—¶é—´: {duration_seconds}ç§’")
        print(f"   SLOé…ç½®: {slo_config}")
        
        slo = self.slo_configs[slo_config]
        print(f"   SLOè¦æ±‚: TTFT<{slo['ttft']}ms, TPOT<{slo['tpot']}ms")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        script_dir = Path(__file__).parent
        project_root = script_dir.parent.parent
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = project_root / "cjworkspace" / "results" / "sglang" / f"continuous_test_{deployment_name}_{concurrency}_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¿è¡ŒæŒç»­æµ‹è¯•
        start_time = time.time()
        test_results = []
        
        try:
            # ä½¿ç”¨genai-perfè¿›è¡ŒæŒç»­æµ‹è¯•
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¿®æ”¹genai.pyä»¥æ”¯æŒæŒç»­æ—¶é—´å‚æ•°
            result = self._run_genai_perf_continuous(
                service_url=service_url,
                model_name=self.model_name,
                concurrency=concurrency,
                duration_seconds=duration_seconds,
                output_dir=output_dir
            )
            
            if result:
                # åˆ†æç»“æœ
                analysis = self._analyze_continuous_results(result, slo, duration_seconds)
                analysis['deployment_name'] = deployment_name
                analysis['service_url'] = service_url
                analysis['concurrency'] = concurrency
                analysis['duration_seconds'] = duration_seconds
                analysis['slo_config'] = slo_config
                analysis['timestamp'] = timestamp
                
                return analysis
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {deployment_name}")
                return None
                
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¼‚å¸¸: {deployment_name} - {str(e)}")
            return None
    
    def _run_genai_perf_continuous(self, 
                                 service_url: str, 
                                 model_name: str, 
                                 concurrency: int, 
                                 duration_seconds: int,
                                 output_dir: Path) -> Optional[Dict]:
        """è¿è¡ŒæŒç»­æ—¶é—´çš„genai-perfæµ‹è¯•"""
        
        # æ„å»ºå‘½ä»¤ - ä½¿ç”¨measurement-intervalæ§åˆ¶æµ‹è¯•æŒç»­æ—¶é—´
        # 3 Ã— measurement-interval = 60000ms (60ç§’)
        cmd = [
            "genai-perf",
            "profile",
            "-m", model_name,
            "--endpoint-type", "chat",
            "--streaming",
            "-u", service_url,
            "--synthetic-input-tokens-mean", "5000",
            "--synthetic-input-tokens-stddev", "0",
            "--concurrency", str(concurrency),
            "--output-tokens-mean", "512",
            "--extra-inputs", "max_tokens:512",
            "--extra-inputs", "min_tokens:512",
            "--extra-inputs", "ignore_eos:true",
            "--tokenizer", "/raid5/models/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
            "--artifact-dir", str(output_dir),
            #"--measurement-interval", "20000",    # æµ‹é‡é—´éš”20ç§’ï¼Œ3Ã—20=60ç§’
            "--request-count", str(concurrency * 4),          # è¶³å¤Ÿå¤§çš„è¯·æ±‚æ•°é‡
            "--",
            "-vv",
            "--max-threads=300",
        ]
        
        print(f"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        try:
            # è¿è¡Œå‘½ä»¤
            process = subprocess.Popen(
                cmd,
                cwd=str(output_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            
            stdout, stderr = process.communicate()  # é¢å¤–2åˆ†é’Ÿè¶…æ—¶
            
            if process.returncode == 0:
                print(f"   âœ… genai-perfæ‰§è¡ŒæˆåŠŸ")
                
                # è¯»å–ç»“æœæ–‡ä»¶
                json_files = list(output_dir.glob("**/profile_export_genai_perf.json"))
                if json_files:
                    with open(json_files[0], 'r') as f:
                        return json.load(f)
                else:
                    print(f"   âš ï¸  æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
                    return None
            else:
                print(f"   âŒ genai-perfæ‰§è¡Œå¤±è´¥ (è¿”å›ç : {process.returncode})")
                if stderr:
                    print(f"   é”™è¯¯ä¿¡æ¯: {stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            print(f"   â° æµ‹è¯•è¶…æ—¶")
            process.kill()
            return None
        except Exception as e:
            print(f"   âŒ æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            return None
    
    def _analyze_continuous_results(self, result_data: Dict, slo: Dict, duration_seconds: int) -> Dict:
        """åˆ†ææŒç»­æµ‹è¯•ç»“æœ"""
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics = {
            'request_throughput_avg': result_data.get('request_throughput', {}).get('avg', 0),
            'request_throughput_p90': result_data.get('request_throughput', {}).get('p90', 0),
            'output_token_throughput_avg': result_data.get('output_token_throughput', {}).get('avg', 0),
            'output_token_throughput_p90': result_data.get('output_token_throughput', {}).get('p90', 0),
            'output_token_throughput_per_user_avg': result_data.get('output_token_throughput_per_user', {}).get('avg', 0),
            'output_token_throughput_per_user_p90': result_data.get('output_token_throughput_per_user', {}).get('p90', 0),
            'ttft_avg': result_data.get('time_to_first_token', {}).get('avg', 0),
            'ttft_p90': result_data.get('time_to_first_token', {}).get('p90', 0),
            'itl_avg': result_data.get('inter_token_latency', {}).get('avg', 0),
            'itl_p90': result_data.get('inter_token_latency', {}).get('p90', 0),
            'request_latency_avg': result_data.get('request_latency', {}).get('avg', 0),
            'request_latency_p90': result_data.get('request_latency', {}).get('p90', 0),
            'request_count': result_data.get('request_count', {}).get('count', 0),
        }
        
        # è®¡ç®—TPOT (Time Per Output Token)
        if metrics['itl_avg'] > 0:
            metrics['tpot_avg'] = metrics['itl_avg']
            metrics['tpot_p90'] = metrics['itl_p90']
        else:
            metrics['tpot_avg'] = 0
            metrics['tpot_p90'] = 0
        
        # SLOæ»¡è¶³ç‡åˆ†æ
        ttft_slo_met = metrics['ttft_p90'] <= slo['ttft']
        tpot_slo_met = metrics['tpot_p90'] <= slo['tpot']
        slo_met = ttft_slo_met and tpot_slo_met
        
        # è®¡ç®—Goodput
        if slo_met:
            goodput = metrics['request_throughput_avg']
            token_goodput = metrics['output_token_throughput_avg']
            token_goodput_per_user = metrics['output_token_throughput_per_user_avg']
        else:
            goodput = 0
            token_goodput = 0
            token_goodput_per_user = 0
        
        # è®¡ç®—æ•ˆç‡æŒ‡æ ‡
        efficiency_metrics = {
            'slo_satisfaction_rate': 1.0 if slo_met else 0.0,
            'ttft_slo_met': ttft_slo_met,
            'tpot_slo_met': tpot_slo_met,
            'goodput': goodput,
            'token_goodput': token_goodput,
            'token_goodput_per_user': token_goodput_per_user,
            'goodput_efficiency': goodput / max(metrics['request_throughput_avg'], 1),
            'requests_per_second': metrics['request_count'] / duration_seconds,
        }
        
        return {
            'metrics': metrics,
            'efficiency': efficiency_metrics,
            'slo_analysis': {
                'slo_met': slo_met,
                'ttft_slo_met': ttft_slo_met,
                'tpot_slo_met': tpot_slo_met,
                'ttft_p90': metrics['ttft_p90'],
                'tpot_p90': metrics['tpot_p90'],
                'ttft_threshold': slo['ttft'],
                'tpot_threshold': slo['tpot'],
            }
        }
    
    def run_multi_deployment_test(self, 
                                deployments: List[Dict], 
                                concurrency: int,
                                duration_seconds: int = 60,
                                slo_config: str = 'moderate',
                                parallel: bool = True) -> Dict:
        """
        è¿è¡Œå¤šéƒ¨ç½²æ¯”è¾ƒæµ‹è¯•
        
        Args:
            deployments: éƒ¨ç½²é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'name': str, 'url': str}
            concurrency: å¹¶å‘æ•°
            duration_seconds: æµ‹è¯•æŒç»­æ—¶é—´
            slo_config: SLOé…ç½®
            parallel: æ˜¯å¦å¹¶è¡Œæµ‹è¯•
        """
        
        print(f"\nğŸ¯ å¼€å§‹å¤šéƒ¨ç½²æ¯”è¾ƒæµ‹è¯•")
        print(f"   éƒ¨ç½²æ•°é‡: {len(deployments)}")
        print(f"   å¹¶å‘æ•°: {concurrency}")
        print(f"   æŒç»­æ—¶é—´: {duration_seconds}ç§’")
        print(f"   SLOé…ç½®: {slo_config}")
        print(f"   å¹¶è¡Œæµ‹è¯•: {'æ˜¯' if parallel else 'å¦'}")
        
        results = {}
        start_time = time.time()
        
        if parallel:
            # å¹¶è¡Œæµ‹è¯•
            with concurrent.futures.ThreadPoolExecutor(max_workers=len(deployments)) as executor:
                future_to_deployment = {
                    executor.submit(
                        self.run_continuous_test,
                        deployment['name'],
                        deployment['url'],
                        concurrency,
                        duration_seconds,
                        slo_config
                    ): deployment for deployment in deployments
                }
                
                for future in concurrent.futures.as_completed(future_to_deployment):
                    deployment = future_to_deployment[future]
                    try:
                        result = future.result()
                        if result:
                            results[deployment['name']] = result
                            print(f"âœ… å®Œæˆæµ‹è¯•: {deployment['name']}")
                        else:
                            print(f"âŒ æµ‹è¯•å¤±è´¥: {deployment['name']}")
                    except Exception as e:
                        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {deployment['name']} - {str(e)}")
        else:
            # ä¸²è¡Œæµ‹è¯•
            for deployment in deployments:
                result = self.run_continuous_test(
                    deployment['name'],
                    deployment['url'],
                    concurrency,
                    duration_seconds,
                    slo_config
                )
                if result:
                    results[deployment['name']] = result
                    print(f"âœ… å®Œæˆæµ‹è¯•: {deployment['name']}")
                else:
                    print(f"âŒ æµ‹è¯•å¤±è´¥: {deployment['name']}")
        
        total_time = time.time() - start_time
        print(f"\nâ±ï¸  æ€»æµ‹è¯•æ—¶é—´: {total_time:.2f}ç§’")
        
        return {
            'test_config': {
                'concurrency': concurrency,
                'duration_seconds': duration_seconds,
                'slo_config': slo_config,
                'parallel': parallel,
                'total_time': total_time,
            },
            'results': results,
            'summary': self._generate_summary(results)
        }
    
    def _generate_summary(self, results: Dict) -> Dict:
        """ç”Ÿæˆæµ‹è¯•ç»“æœæ‘˜è¦"""
        
        if not results:
            return {'error': 'No results to summarize'}
        
        summary = {
            'deployment_count': len(results),
            'successful_tests': len([r for r in results.values() if r is not None]),
            'slo_satisfaction': {},
            'performance_ranking': {},
            'best_performers': {}
        }
        
        # SLOæ»¡è¶³ç‡ç»Ÿè®¡
        slo_satisfied = [name for name, result in results.items() 
                        if result and result['slo_analysis']['slo_met']]
        summary['slo_satisfaction'] = {
            'satisfied_count': len(slo_satisfied),
            'satisfied_deployments': slo_satisfied,
            'satisfaction_rate': len(slo_satisfied) / len(results)
        }
        
        # æ€§èƒ½æ’å
        goodput_ranking = sorted(
            [(name, result['efficiency']['goodput']) for name, result in results.items() if result],
            key=lambda x: x[1],
            reverse=True
        )
        summary['performance_ranking'] = {
            'by_goodput': goodput_ranking,
            'by_token_goodput': sorted(
                [(name, result['efficiency']['token_goodput']) for name, result in results.items() if result],
                key=lambda x: x[1],
                reverse=True
            )
        }
        
        # æœ€ä½³è¡¨ç°è€…
        if goodput_ranking:
            summary['best_performers'] = {
                'highest_goodput': goodput_ranking[0],
                'lowest_latency': min(
                    [(name, result['metrics']['ttft_p90']) for name, result in results.items() if result],
                    key=lambda x: x[1]
                ) if results else None
            }
        
        return summary
    
    def save_results(self, results: Dict, output_file: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"continuous_benchmark_results_{timestamp}.json"
        
        output_path = Path(output_file)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def generate_report(self, results: Dict, output_file: str = None):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"continuous_benchmark_report_{timestamp}.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("æŒç»­æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 80 + "\n\n")
            
            # æµ‹è¯•é…ç½®
            config = results['test_config']
            f.write("æµ‹è¯•é…ç½®:\n")
            f.write(f"  å¹¶å‘æ•°: {config['concurrency']}\n")
            f.write(f"  æŒç»­æ—¶é—´: {config['duration_seconds']}ç§’\n")
            f.write(f"  SLOé…ç½®: {config['slo_config']}\n")
            f.write(f"  å¹¶è¡Œæµ‹è¯•: {'æ˜¯' if config['parallel'] else 'å¦'}\n")
            f.write(f"  æ€»æµ‹è¯•æ—¶é—´: {config['total_time']:.2f}ç§’\n\n")
            
            # æ‘˜è¦ä¿¡æ¯
            summary = results['summary']
            f.write("æµ‹è¯•æ‘˜è¦:\n")
            f.write(f"  éƒ¨ç½²æ•°é‡: {summary['deployment_count']}\n")
            f.write(f"  æˆåŠŸæµ‹è¯•: {summary['successful_tests']}\n")
            f.write(f"  SLOæ»¡è¶³ç‡: {summary['slo_satisfaction']['satisfaction_rate']:.2%}\n")
            f.write(f"  æ»¡è¶³SLOçš„éƒ¨ç½²: {', '.join(summary['slo_satisfaction']['satisfied_deployments'])}\n\n")
            
            # æ€§èƒ½æ’å
            f.write("æ€§èƒ½æ’å (æŒ‰Goodput):\n")
            for i, (name, goodput) in enumerate(summary['performance_ranking']['by_goodput'], 1):
                f.write(f"  {i}. {name}: {goodput:.2f} req/s\n")
            f.write("\n")
            
            # è¯¦ç»†ç»“æœ
            f.write("è¯¦ç»†ç»“æœ:\n")
            f.write("-" * 80 + "\n")
            for name, result in results['results'].items():
                if result:
                    f.write(f"\néƒ¨ç½²: {name}\n")
                    f.write(f"  æœåŠ¡URL: {result['service_url']}\n")
                    f.write(f"  SLOæ»¡è¶³: {'æ˜¯' if result['slo_analysis']['slo_met'] else 'å¦'}\n")
                    f.write(f"  TTFT P90: {result['metrics']['ttft_p90']:.2f}ms\n")
                    f.write(f"  TPOT P90: {result['metrics']['tpot_p90']:.2f}ms\n")
                    f.write(f"  è¯·æ±‚ååç‡: {result['metrics']['request_throughput_avg']:.2f} req/s\n")
                    f.write(f"  Tokenååç‡: {result['metrics']['output_token_throughput_avg']:.2f} tokens/s\n")
                    f.write(f"  Goodput: {result['efficiency']['goodput']:.2f} req/s\n")
                    f.write(f"  Token Goodput: {result['efficiency']['token_goodput']:.2f} tokens/s\n")
                else:
                    f.write(f"\néƒ¨ç½²: {name} - æµ‹è¯•å¤±è´¥\n")
        
        print(f"ğŸ“Š æŠ¥å‘Šå·²ç”Ÿæˆ: {output_file}")
        return output_file


def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    benchmark = ContinuousBenchmark()
    
    # å®šä¹‰è¦æµ‹è¯•çš„éƒ¨ç½²
    deployments = [
        {
            'name': 'sglang-agg',
            'url': 'http://127.0.0.1:8003'
        },
        {
            'name': 'sglang-disagg',
            'url': 'http://127.0.0.1:8005'
        }
    ]
    
    # æµ‹è¯•é…ç½®
    concurrencies = list(range(1, 31, 2))  # ä»1åˆ°30ï¼Œæ¯æ¬¡å¢åŠ 2: [1, 3, 5, 7, ..., 29]
    duration_seconds = 60  # æŒç»­æ—¶é—´
    slo_config = 'moderate'  # SLOé…ç½®
    parallel = True  # æ˜¯å¦å¹¶è¡Œæµ‹è¯•
    
    print("ğŸš€ å¼€å§‹æŒç»­æ€§èƒ½æµ‹è¯• - å¹¶å‘æ•°æ‰«æ")
    print(f"æµ‹è¯•é…ç½®: å¹¶å‘æ•°èŒƒå›´={concurrencies}, æŒç»­æ—¶é—´={duration_seconds}ç§’, SLO={slo_config}")
    print(f"éƒ¨ç½²: {[d['name'] for d in deployments]}")
    
    # è¿è¡Œå¹¶å‘æ•°æ‰«ææµ‹è¯•
    all_results = {}
    for concurrency in concurrencies:
        print(f"\nğŸ“Š æµ‹è¯•å¹¶å‘æ•°: {concurrency}")
        
        results = benchmark.run_multi_deployment_test(
            deployments=deployments,
            concurrency=concurrency,
            duration_seconds=duration_seconds,
            slo_config=slo_config,
            parallel=parallel
        )
        
        if results:
            all_results[f'concurrency_{concurrency}'] = results
            
            # æ‰“å°å½“å‰å¹¶å‘æ•°çš„æ‘˜è¦
            summary = results['summary']
            if 'successful_tests' in summary:
                print(f"  âœ… å®Œæˆæµ‹è¯•: æˆåŠŸ={summary['successful_tests']}/{summary['deployment_count']}")
                print(f"  ğŸ“ˆ SLOæ»¡è¶³ç‡: {summary['slo_satisfaction']['satisfaction_rate']:.2%}")
            else:
                print("  âš ï¸ æœ¬è½®æ²¡æœ‰æˆåŠŸç»“æœï¼ˆå¯èƒ½å‘½ä»¤è¡Œå‚æ•°é”™è¯¯æˆ–æœåŠ¡ä¸å¯è¾¾ï¼‰")
            
            # æ˜¾ç¤ºæ¯ä¸ªéƒ¨ç½²çš„è¯¦ç»†æŒ‡æ ‡
            for deployment_name, result in results['results'].items():
                if result:
                    metrics = result['metrics']
                    slo_analysis = result['slo_analysis']
                    print(f"    ğŸ“Š {deployment_name}:")
                    print(f"      TTFT P90: {metrics['ttft_p90']:.2f}ms (SLO: {slo_analysis['ttft_slo_met']})")
                    print(f"      ITL P90: {metrics['itl_p90']:.2f}ms (SLO: {slo_analysis['tpot_slo_met']})")
                    print(f"      Goodput: {result['efficiency']['goodput']:.2f} req/s")
                    print(f"      Token Goodput: {result['efficiency']['token_goodput']:.2f} tokens/s")
            
            if 'performance_ranking' in summary and summary['performance_ranking'].get('by_goodput'):
                best = summary['performance_ranking']['by_goodput'][0]
                print(f"  ğŸ† æœ€ä½³æ€§èƒ½: {best[0]} (Goodput: {best[1]:.2f} req/s)")
        else:
            print(f"  âŒ æµ‹è¯•å¤±è´¥")
    
    # ä¿å­˜æ±‡æ€»ç»“æœ
    if all_results:
        json_file = benchmark.save_results(all_results, 'concurrency_scan_results.json')
        print(f"\nğŸ’¾ æ±‡æ€»ç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        # ç”Ÿæˆæœ€ç»ˆæ‘˜è¦
        print(f"\nğŸ“ˆ å¹¶å‘æ•°æ‰«æå®Œæˆ!")
        print(f"æµ‹è¯•çš„å¹¶å‘æ•°: {concurrencies}")
        print(f"æˆåŠŸå®Œæˆçš„æµ‹è¯•: {len(all_results)}/{len(concurrencies)}")
        
        # æ‰¾å‡ºæœ€ä½³å¹¶å‘æ•°å’Œç›¸å…³ç»Ÿè®¡
        best_concurrency = None
        best_goodput = 0
        latency_stats = {}
        
        for concurrency in concurrencies:
            key = f'concurrency_{concurrency}'
            if key in all_results:
                results = all_results[key]
                if results['summary']['performance_ranking']['by_goodput']:
                    goodput = results['summary']['performance_ranking']['by_goodput'][0][1]
                    if goodput > best_goodput:
                        best_goodput = goodput
                        best_concurrency = concurrency
                
                # æ”¶é›†å»¶è¿Ÿç»Ÿè®¡
                for deployment_name, result in results['results'].items():
                    if result:
                        if deployment_name not in latency_stats:
                            latency_stats[deployment_name] = {
                                'ttft_p90_values': [],
                                'itl_p90_values': [],
                                'goodput_values': []
                            }
                        latency_stats[deployment_name]['ttft_p90_values'].append(result['metrics']['ttft_p90'])
                        latency_stats[deployment_name]['itl_p90_values'].append(result['metrics']['itl_p90'])
                        latency_stats[deployment_name]['goodput_values'].append(result['efficiency']['goodput'])
        
        if best_concurrency:
            print(f"ğŸ† æœ€ä½³å¹¶å‘æ•°: {best_concurrency} (Goodput: {best_goodput:.2f} req/s)")
        
        # æ˜¾ç¤ºå»¶è¿Ÿç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“Š å»¶è¿Ÿç»Ÿè®¡æ‘˜è¦:")
        for deployment_name, stats in latency_stats.items():
            if stats['ttft_p90_values']:
                avg_ttft = sum(stats['ttft_p90_values']) / len(stats['ttft_p90_values'])
                min_ttft = min(stats['ttft_p90_values'])
                max_ttft = max(stats['ttft_p90_values'])
                
                avg_itl = sum(stats['itl_p90_values']) / len(stats['itl_p90_values'])
                min_itl = min(stats['itl_p90_values'])
                max_itl = max(stats['itl_p90_values'])
                
                avg_goodput = sum(stats['goodput_values']) / len(stats['goodput_values'])
                max_goodput = max(stats['goodput_values'])
                
                print(f"  ğŸ“ˆ {deployment_name}:")
                print(f"    TTFT P90: å¹³å‡={avg_ttft:.2f}ms, æœ€å°={min_ttft:.2f}ms, æœ€å¤§={max_ttft:.2f}ms")
                print(f"    ITL P90: å¹³å‡={avg_itl:.2f}ms, æœ€å°={min_itl:.2f}ms, æœ€å¤§={max_itl:.2f}ms")
                print(f"    Goodput: å¹³å‡={avg_goodput:.2f} req/s, æœ€å¤§={max_goodput:.2f} req/s")
    else:
        print(f"\nâŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•ç»“æœ")


if __name__ == "__main__":
    main()