#!/usr/bin/env python3
"""
ä» distserve_agg.py ç”Ÿæˆçš„æµ‹è¯•ç»“æœä¸­æå–æ‰€æœ‰æŒ‡æ ‡ï¼Œæ±‡æ€»åˆ° CSV è¡¨æ ¼

ç”¨æ³•:
    python extract_distserve_metrics.py [--output-dir OUTPUT_DIR] [--output-csv OUTPUT_CSV]
"""

import json
import csv
import glob
import os
import sys
import re
from pathlib import Path
from typing import Dict, List, Any
import argparse


def get_isl_osl_from_json(json_file: str) -> tuple[float, float]:
    """
    ä»JSONæ–‡ä»¶ä¸­è¯»å–ISLå’ŒOSL
    
    Returns:
        (isl, osl) å…ƒç»„ï¼Œå¦‚æœè¯»å–å¤±è´¥åˆ™è¿”å› (None, None)
    """
    try:
        with open(json_file, 'r') as f:
            data = json.load(f)
        
        isl = None
        osl = None
        
        # å°è¯•ä»input_sequence_lengthå’Œoutput_sequence_lengthä¸­è¯»å–avgå€¼
        if 'input_sequence_length' in data:
            isl_dict = data['input_sequence_length']
            if isinstance(isl_dict, dict) and 'avg' in isl_dict:
                isl = float(isl_dict['avg'])
        
        if 'output_sequence_length' in data:
            osl_dict = data['output_sequence_length']
            if isinstance(osl_dict, dict) and 'avg' in osl_dict:
                osl = float(osl_dict['avg'])
        
        return isl, osl
    except Exception as e:
        print(f"âš ï¸  Warning: Failed to read ISL/OSL from {json_file}: {e}")
        return None, None


def find_result_files(base_dir: str = None, mode: str = "agg", filter_isl: float = None, filter_osl: float = None, filter_deployment: str = None) -> Dict[tuple, tuple]:
    """
    æŸ¥æ‰¾æ‰€æœ‰å¹¶å‘åº¦ä¸‹çš„ profile_export_genai_perf.json æ–‡ä»¶ï¼ŒæŒ‰ (concurrency, isl, osl, deployment_name) ç»„åˆåŒºåˆ†
    
    Args:
        base_dir: åŸºç¡€ç›®å½•ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨ cjworkspace/tempï¼‰
        mode: "agg" æˆ– "disagg" æ¨¡å¼
        filter_isl: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šçš„ISLå€¼
        filter_osl: å¯é€‰ï¼Œè¿‡æ»¤ç‰¹å®šçš„OSLå€¼
    
    Returns:
        Dict[(concurrency, isl, osl, deployment_name), (json_file_path, deployment_name)]
    """
    result_files = {}
    
    # å¦‚æœæœªæŒ‡å®šbase_dirï¼Œä½¿ç”¨cjworkspace/temp
    if base_dir is None:
        script_dir = Path(__file__).parent
        # ä» cjworkspace/analysis/plot å¾€ä¸Šä¸¤çº§åˆ° cjworkspace
        cjworkspace_dir = script_dir.parent.parent
        base_dir = str(cjworkspace_dir / "temp")
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©ç›®å½•æ¨¡å¼
    # æ–°æ ¼å¼: {mode}_{deployment_name}_isl{isl}_osl{osl}_concurrency{concurrency}
    # ä¹Ÿæ”¯æŒæ—§æ ¼å¼: {mode}_isl{isl}_osl{osl}_concurrency{concurrency}ï¼ˆå‘åå…¼å®¹ï¼‰
    if mode == "disagg":
        pattern = os.path.join(base_dir, "disagg*_isl*_osl*_concurrency*")
    else:
        pattern = os.path.join(base_dir, "agg*_isl*_osl*_concurrency*")
    
    test_dirs = glob.glob(pattern)
    
    for test_dir in test_dirs:
        # ä»ç›®å½•åæå–ä¿¡æ¯
        dir_name = os.path.basename(test_dir)
        
        # å°è¯•åŒ¹é…æ–°æ ¼å¼: {mode}_{deployment_name}_isl{isl}_osl{osl}_concurrency{concurrency}
        # ä½¿ç”¨éè´ªå©ªåŒ¹é…æ¥æ•è·å¯èƒ½åŒ…å«ä¸‹åˆ’çº¿çš„éƒ¨ç½²åç§°
        match = re.match(rf'{mode}_(.+?)_isl(\d+(?:\.\d+)?)_osl(\d+(?:\.\d+)?)_concurrency(\d+)', dir_name)
        deployment_name = None
        if match:
            deployment_name = match.group(1)
            isl = float(match.group(2))
            osl = float(match.group(3))
            concurrency = int(match.group(4))
        else:
            # å°è¯•åŒ¹é…æ—§æ ¼å¼: {mode}_isl{isl}_osl{osl}_concurrency{concurrency}ï¼ˆå‘åå…¼å®¹ï¼‰
            match = re.match(rf'{mode}_isl(\d+(?:\.\d+)?)_osl(\d+(?:\.\d+)?)_concurrency(\d+)', dir_name)
            if match:
                deployment_name = mode  # ä½¿ç”¨é»˜è®¤å€¼
                isl = float(match.group(1))
                osl = float(match.group(2))
                concurrency = int(match.group(3))
            else:
                # å¦‚æœæ— æ³•è§£æï¼Œå°è¯•ä»JSONæ–‡ä»¶è¯»å–
                json_pattern = os.path.join(test_dir, "**", "profile_export_genai_perf.json")
                json_files = glob.glob(json_pattern, recursive=True)
                if json_files:
                    json_file = json_files[0]
                    isl, osl = get_isl_osl_from_json(json_file)
                    if isl is None or osl is None:
                        continue
                    # å°è¯•ä»ç›®å½•åä¸­æå–å¹¶å‘åº¦
                    concurrency_match = re.search(r'concurrency(\d+)', dir_name)
                    if concurrency_match:
                        concurrency = int(concurrency_match.group(1))
                        deployment_name = mode  # ä½¿ç”¨é»˜è®¤å€¼
                    else:
                        continue
                else:
                    continue
        
        # å¦‚æœæŒ‡å®šäº†è¿‡æ»¤å™¨ï¼Œæ£€æŸ¥æ˜¯å¦ç¬¦åˆ
        if filter_isl is not None and abs(isl - filter_isl) > 0.1:
            continue
        if filter_osl is not None and abs(osl - filter_osl) > 0.1:
            continue
        if filter_deployment:
            # æ”¯æŒç”¨é€—å·åˆ†éš”çš„å¤šä¸ªéƒ¨ç½²åï¼›å¤§å°å†™ä¸æ•æ„Ÿï¼Œç²¾ç¡®åŒ¹é…æˆ–å‰ç¼€åŒ¹é…
            wanted = [d.strip().lower() for d in filter_deployment.split(',') if d.strip()]
            name_lc = (deployment_name or '').lower()
            # ç²¾ç¡®åŒ¹é…æˆ–ä½œä¸ºå‰ç¼€åŒ¹é…ï¼ˆä¾‹å¦‚ "3p1d" åŒ¹é… "3p1d" æˆ– "3p1d_xxx"ï¼‰
            if wanted and not any(name_lc == w or name_lc.startswith(w + '_') for w in wanted):
                continue
        
        # æŸ¥æ‰¾ JSON æ–‡ä»¶
        json_pattern = os.path.join(test_dir, "**", "profile_export_genai_perf.json")
        json_files = glob.glob(json_pattern, recursive=True)
        
        if not json_files:
            continue
        
        # ä½¿ç”¨æœ€æ–°çš„JSONæ–‡ä»¶
        json_file = max(json_files, key=lambda x: os.path.getmtime(x))
        
        # éªŒè¯JSONæ–‡ä»¶ä¸­çš„ISL/OSLæ˜¯å¦ä¸ç›®å½•ååŒ¹é…
        json_isl, json_osl = get_isl_osl_from_json(json_file)
        if json_isl is not None and json_osl is not None:
            # å¦‚æœJSONä¸­çš„å€¼ä¸ç›®å½•åä¸åŒ¹é…ï¼Œä½¿ç”¨JSONä¸­çš„å€¼
            if abs(json_isl - isl) > 0.1 or abs(json_osl - osl) > 0.1:
                isl = json_isl
                osl = json_osl
        
        # ä½¿ç”¨ (concurrency, isl, osl, deployment_name) ä½œä¸ºé”®
        key = (concurrency, isl, osl, deployment_name)
        
        # å¦‚æœåŒä¸€ä¸ªç»„åˆæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°çš„
        if key in result_files:
            existing_file = result_files[key][0]
            existing_mtime = os.path.getmtime(existing_file)
            current_mtime = os.path.getmtime(json_file)
            if current_mtime > existing_mtime:
                result_files[key] = (json_file, deployment_name)
                print(f"âœ… Updated concurrency {concurrency} ISL={isl:.0f} OSL={osl:.0f} deployment={deployment_name}: {json_file}")
        else:
            result_files[key] = (json_file, deployment_name)
            print(f"âœ… Found concurrency {concurrency} ISL={isl:.0f} OSL={osl:.0f} deployment={deployment_name}: {json_file}")
    
    return result_files


def extract_all_metrics(json_file: str) -> Dict[str, Any]:
    """
    ä» JSON æ–‡ä»¶ä¸­æå–æ‰€æœ‰å¯ç”¨çš„æŒ‡æ ‡
    
    Returns:
        åŒ…å«æ‰€æœ‰æŒ‡æ ‡å’Œç»Ÿè®¡å€¼çš„å­—å…¸
    """
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    metrics = {}
    
    # ç»Ÿè®¡å€¼ç±»å‹ï¼ˆæŒ‰å¸¸è§é¡ºåºï¼‰
    stat_types = ['avg', 'min', 'max', 'median', 'p50', 'p90', 'p95', 'p99', 'std', 'count']
    
    def extract_metric_dict(prefix: str, metric_dict: Dict[str, Any]):
        """ä»å­—å…¸ä¸­æå–æŒ‡æ ‡ï¼Œå­—å…¸é€šå¸¸åŒ…å«ç»Ÿè®¡å€¼å¦‚ avg, p90 ç­‰"""
        # æå–æ‰€æœ‰ç»Ÿè®¡å€¼
        for stat in stat_types:
            if stat in metric_dict:
                stat_key = f"{prefix}_{stat}" if prefix else stat
                metrics[stat_key] = metric_dict[stat]
        
        # å¦‚æœå­˜åœ¨åŸå§‹æ•°æ®æ•°ç»„ï¼Œè®°å½•æ•°æ®ç‚¹æ•°é‡
        if 'data' in metric_dict and isinstance(metric_dict['data'], list):
            data_list = metric_dict['data']
            if data_list:
                metrics[f"{prefix}_data_count"] = len(data_list)
    
    # å¤„ç†é¡¶çº§å­—æ®µ
    for key, value in data.items():
        if isinstance(value, dict):
            # æ£€æŸ¥æ˜¯å¦æ˜¯åŒ…å«ç»Ÿè®¡å€¼çš„å­—å…¸ï¼ˆå¦‚ time_to_first_token, inter_token_latencyï¼‰
            if any(stat in value for stat in stat_types):
                extract_metric_dict(key, value)
            else:
                # å¯èƒ½æ˜¯åµŒå¥—ç»“æ„ï¼Œé€’å½’å¤„ç†
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, dict) and any(stat in sub_value for stat in stat_types):
                        extract_metric_dict(f"{key}_{sub_key}", sub_value)
                    elif isinstance(sub_value, (int, float, str)):
                        metrics[f"{key}_{sub_key}"] = sub_value
        elif isinstance(value, (int, float, str)):
            metrics[key] = value
        elif isinstance(value, list):
            metrics[f"{key}_count"] = len(value)
    
    return metrics


def collect_all_metrics(result_files: Dict[tuple, tuple]) -> List[Dict[str, Any]]:
    """
    æ”¶é›†æ‰€æœ‰å¹¶å‘åº¦çš„æŒ‡æ ‡æ•°æ®ï¼ˆæ”¯æŒæŒ‰ISL/OSL/éƒ¨ç½²åç§°åŒºåˆ†ï¼‰
    
    Args:
        result_files: Dict[(concurrency, isl, osl, deployment_name), (json_file_path, deployment_name)]
    
    Returns:
        åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸åˆ—è¡¨
    """
    all_results = []
    
    # æŒ‰ (concurrency, isl, osl, deployment_name) æ’åº
    for key in sorted(result_files.keys()):
        concurrency, isl, osl, deployment_name = key
        json_file, deployment_name_from_dict = result_files[key]
        
        try:
            metrics = extract_all_metrics(json_file)
            metrics['concurrency'] = concurrency
            metrics['deployment_name'] = deployment_name_from_dict or deployment_name or 'unknown'
            # ç¡®ä¿ISLå’ŒOSLè¢«åŒ…å«ï¼ˆå¦‚æœextract_all_metricsæ²¡æœ‰æå–åˆ°ï¼‰
            if 'input_sequence_length_avg' not in metrics or metrics['input_sequence_length_avg'] == 0:
                metrics['input_sequence_length_avg'] = isl
            if 'output_sequence_length_avg' not in metrics or metrics['output_sequence_length_avg'] == 0:
                metrics['output_sequence_length_avg'] = osl
            all_results.append(metrics)
            print(f"âœ… Extracted metrics for concurrency {concurrency} ISL={isl:.0f} OSL={osl:.0f} deployment={metrics['deployment_name']}")
        except Exception as e:
            print(f"âŒ Error extracting metrics for concurrency {concurrency} ISL={isl:.0f} OSL={osl:.0f}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    return all_results


def get_all_columns(all_results: List[Dict[str, Any]]) -> List[str]:
    """
    è·å–æ‰€æœ‰å¯èƒ½çš„åˆ—åï¼ˆæŒ‰å›ºå®šé¡ºåºï¼‰
    """
    # å®šä¹‰åˆ—çš„é¡ºåºï¼šå…ˆ concurrency, deployment_name, ISL, OSLï¼Œç„¶åæ˜¯å„ç§æŒ‡æ ‡çš„ç»Ÿè®¡å€¼
    column_order = ['concurrency', 'deployment_name', 'input_sequence_length_avg', 'output_sequence_length_avg']
    
    # å®šä¹‰æŒ‡æ ‡çš„ä¼˜å…ˆçº§é¡ºåº
    metric_order = [
        'time_to_first_token',
        'inter_token_latency',
        'request_latency',
        'prefill_latency',
        'decode_latency',
        'request_throughput',
        'output_token_throughput',
        'output_token_throughput_per_user',
        'input_token_count',
        'output_token_count',
        'total_token_count',
    ]
    
    # ç»Ÿè®¡å€¼çš„é¡ºåº
    stat_order = ['avg', 'min', 'max', 'median', 'p50', 'p90', 'p95', 'p99', 'std', 'count']
    
    # æ„å»ºåˆ—å
    for metric in metric_order:
        for stat in stat_order:
            column_order.append(f"{metric}_{stat}")
    
    # æ·»åŠ å…¶ä»–å¯èƒ½å­˜åœ¨çš„åˆ—
    all_keys = set()
    for result in all_results:
        all_keys.update(result.keys())
    
    # æ·»åŠ æœªåœ¨é¢„è®¾é¡ºåºä¸­çš„åˆ—
    other_columns = sorted([k for k in all_keys if k not in column_order])
    
    # ç¡®ä¿æ‰€æœ‰åˆ—éƒ½è¢«åŒ…å«ï¼Œä½†ä¿æŒé¢„è®¾é¡ºåº
    final_columns = column_order + other_columns
    
    # åªè¿”å›å®é™…å­˜åœ¨çš„åˆ—
    existing_columns = [col for col in final_columns if any(col in result for result in all_results)]
    
    return existing_columns


def write_csv(results: List[Dict[str, Any]], output_file: str):
    """
    å°†ç»“æœå†™å…¥ CSV æ–‡ä»¶
    """
    if not results:
        print("âŒ No results to write")
        return
    
    columns = get_all_columns(results)
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=columns)
        writer.writeheader()
        
        for result in results:
            row = {col: result.get(col, '') for col in columns}
            writer.writerow(row)
    
    print(f"âœ… CSV file written: {output_file}")
    print(f"   Total rows: {len(results)}")
    print(f"   Total columns: {len(columns)}")


def main():
    parser = argparse.ArgumentParser(
        description='Extract all metrics from distserve_agg.py test results to CSV'
    )
    parser.add_argument(
        '--base-dir',
        type=str,
        default=None,
        help='Base directory to search for test results (default: cjworkspace/temp relative to project root)'
    )
    parser.add_argument(
        '--output-csv',
        type=str,
        default=None,
        help='Output CSV file path (default: distserve_metrics_TIMESTAMP.csv)'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='Output directory for CSV file (default: current directory)'
    )
    parser.add_argument(
        '--mode',
        type=str,
        choices=['agg', 'disagg'],
        default='agg',
        help='Test mode: agg (aggregated) or disagg (disaggregated) (default: agg)'
    )
    parser.add_argument(
        '--filter-isl',
        type=float,
        default=None,
        help='Filter by specific input sequence length (ISL). If not specified, includes all ISL values.'
    )
    parser.add_argument(
        '--filter-osl',
        type=float,
        default=None,
        help='Filter by specific output sequence length (OSL). If not specified, includes all OSL values.'
    )
    parser.add_argument(
        '--filter-deployment',
        type=str,
        default=None,
        help='Filter by deployment name (supports comma-separated values, case-insensitive, substring match).'
    )
    
    args = parser.parse_args()
    
    print("ğŸ” Searching for test result files...")
    print(f"   Base directory: {args.base_dir}")
    print(f"   Mode: {args.mode}")
    if args.filter_isl is not None:
        print(f"   Filter ISL: {args.filter_isl}")
    if args.filter_osl is not None:
        print(f"   Filter OSL: {args.filter_osl}")
    if args.filter_deployment:
        print(f"   Filter deployment: {args.filter_deployment}")
    
    # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶ï¼ˆæŒ‰ (concurrency, isl, osl) ç»„åˆåŒºåˆ†ï¼‰
    result_files = find_result_files(args.base_dir, args.mode, args.filter_isl, args.filter_osl, args.filter_deployment)
    
    if not result_files:
        print("âŒ No result files found!")
        if args.base_dir:
            search_dir = args.base_dir
        else:
            script_dir = Path(__file__).parent
            # ä» cjworkspace/analysis/plot å¾€ä¸Šä¸¤çº§åˆ° cjworkspace
            cjworkspace_dir = script_dir.parent.parent
            search_dir = str(cjworkspace_dir / "temp")
        
        if args.mode == "disagg":
            print(f"   Searched in: {search_dir}/disagg_isl*_osl*_concurrency*")
        else:
            print(f"   Searched in: {search_dir}/agg_isl*_osl*_concurrency*")
        sys.exit(1)
    
    # ç»Ÿè®¡ä¿¡æ¯
    unique_concurrencies = set()
    isl_osl_combinations = set()
    deployment_names = set()
    for key in result_files.keys():
        concurrency, isl, osl, deployment_name = key
        unique_concurrencies.add(concurrency)
        isl_osl_combinations.add((isl, osl))
        deployment_names.add(deployment_name)
    
    print(f"\nâœ… Found {len(result_files)} test results")
    print(f"   Unique concurrency levels: {sorted(unique_concurrencies)}")
    print(f"   ISL/OSL combinations: {sorted(isl_osl_combinations)}")
    print(f"   Deployment names: {sorted(deployment_names)}")
    if len(isl_osl_combinations) > 1:
        print(f"   âš ï¸  Note: Multiple ISL/OSL combinations found. All will be included in the CSV.")
    if len(deployment_names) > 1:
        print(f"   âš ï¸  Note: Multiple deployment names found. All will be included in the CSV.")
    
    # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡
    print("\nğŸ“Š Extracting metrics...")
    all_results = collect_all_metrics(result_files)
    
    if not all_results:
        print("âŒ No metrics extracted!")
        sys.exit(1)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    if args.output_csv:
        output_file = args.output_csv
    else:
        from datetime import datetime
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"distserve_metrics_{args.mode}_{timestamp}.csv"
    
    if args.output_dir:
        os.makedirs(args.output_dir, exist_ok=True)
        output_file = os.path.join(args.output_dir, os.path.basename(output_file))
    
    # å†™å…¥ CSV
    print(f"\nğŸ’¾ Writing CSV file...")
    write_csv(all_results, output_file)
    
    print(f"\nâœ… Done! Results saved to: {output_file}")
    
    # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    if all_results:
        print(f"\nğŸ“ˆ Summary:")
        print(f"   Total data points: {len(all_results)}")
        
        # ç»Ÿè®¡å”¯ä¸€çš„å¹¶å‘åº¦æ•°é‡å’ŒISL/OSLç»„åˆ
        unique_concurrencies = set(r['concurrency'] for r in all_results)
        unique_isl_osl = set((r.get('input_sequence_length_avg', 0), r.get('output_sequence_length_avg', 0)) for r in all_results)
        
        print(f"   Unique concurrency levels: {len(unique_concurrencies)}")
        print(f"   Concurrency range: {min(unique_concurrencies)} - {max(unique_concurrencies)}")
        print(f"   ISL/OSL combinations: {len(unique_isl_osl)}")
        for isl, osl in sorted(unique_isl_osl):
            count = sum(1 for r in all_results if abs(r.get('input_sequence_length_avg', 0) - isl) < 0.1 and 
                       abs(r.get('output_sequence_length_avg', 0) - osl) < 0.1)
            print(f"      ISL={isl:.0f} OSL={osl:.0f}: {count} data points")
        
        # æ˜¾ç¤ºå¯ç”¨æŒ‡æ ‡çš„ç¤ºä¾‹
        sample = all_results[0]
        key_metrics = [k for k in sample.keys() if k not in ['concurrency', 'input_sequence_length_avg', 'output_sequence_length_avg']]
        print(f"   Available metrics: {len(key_metrics)}")
        print(f"   Sample metrics: {', '.join(sorted(key_metrics)[:10])}...")


if __name__ == '__main__':
    main()

