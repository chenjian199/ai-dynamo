#!/usr/bin/env python3
"""
åŸºäºDistServeè®ºæ–‡ç†è®ºçš„PDåˆ†ç¦»æ€§èƒ½æµ‹è¯•
å®ç°SLOçº¦æŸä¸‹çš„Goodputæµ‹è¯•
"""

import json
import subprocess
import time
import os
import sys
from typing import List, Dict, Tuple
import statistics
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/bedicloud/dynamo-main/benchmarks/utils')
from genai import run_genai_perf

class DistServeStyleTest:
    """åŸºäºDistServeç†è®ºçš„æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, service_url: str = None, model_name: str = None):
        self.results = {}
        
        # æœåŠ¡URLå’Œæ¨¡å‹åç§°é…ç½®ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        self.service_url = service_url or os.environ.get('SERVICE_URL', 'http://127.0.0.1:8004')
        self.model_name = model_name or os.environ.get('DEPLOYMENT_MODEL_ID', 'DeepSeek-R1-Distill-Qwen-7B')
        
        self.slo_configs = {
            # åŸºäºå®é™…æ•°æ®åˆ†æçš„SLOé…ç½®
            'ultra_strict': {'ttft': 50, 'tpot': 8},      # è¶…ä¸¥æ ¼SLO (P50æ°´å¹³)
            'strict': {'ttft': 100, 'tpot': 12},          # ä¸¥æ ¼SLO (P75æ°´å¹³)
            'moderate': {'ttft': 200, 'tpot': 15},        # ä¸­ç­‰SLO (P90æ°´å¹³)
            'loose': {'ttft': 400, 'tpot': 20},           # å®½æ¾SLO (P95æ°´å¹³)
            'very_loose': {'ttft': 800, 'tpot': 30},      # å¾ˆå®½æ¾SLO (P99æ°´å¹³)
            #
        }
        
    def run_benchmark_with_slo(self, concurrency: int, slo_config: str) -> Dict:
        """è¿è¡Œå¸¦SLOçº¦æŸçš„åŸºå‡†æµ‹è¯•"""
        
        slo = self.slo_configs[slo_config]
        print(f"Testing concurrency {concurrency} with {slo_config} SLO (TTFT<{slo['ttft']}ms, TPOT<{slo['tpot']}ms)")
        
        # è¿è¡Œgenai-perfæµ‹è¯•
        result = run_genai_perf(
            service_url=self.service_url,
            model_name=self.model_name,
            isl=2000,
            osl=2000,
            stddev=0,
            concurrency=concurrency,
            output_dir=Path(f"/tmp/distserve_test_{concurrency}")
        )
        
        if not result:
            return None
            
        # ä»è¾“å‡ºç›®å½•è¯»å–ç»“æœ
        output_dir = Path(f"/tmp/distserve_test_{concurrency}")
        json_files = list(output_dir.glob("**/profile_export_genai_perf.json"))
        
        if not json_files:
            print(f"Warning: No results found in {output_dir}")
            return None
            
        # è¯»å–ç¬¬ä¸€ä¸ªç»“æœæ–‡ä»¶
        with open(json_files[0], 'r') as f:
            result_data = json.load(f)
            
        # åˆ†æSLOæ»¡è¶³ç‡
        slo_analysis = self.analyze_slo_satisfaction(result_data, slo)
        
        # è®¡ç®—è‰¯è¯·æ±‚ååç‡ï¼ˆGoodputï¼‰
        total_throughput = result_data.get('request_throughput', {}).get('avg', 0)
        output_token_throughput = result_data.get('output_token_throughput', {}).get('avg', 0)
        output_token_throughput_per_user = result_data.get('output_token_throughput_per_user', {}).get('avg', 0)
        
        # å¦‚æœP90æ»¡è¶³SLOï¼Œåˆ™Goodputå°±æ˜¯æ€»ååç‡ï¼ˆ90%çš„è¯·æ±‚æ»¡è¶³SLOï¼‰
        # å¦‚æœP90ä¸æ»¡è¶³SLOï¼Œåˆ™Goodputä¸º0
        if slo_analysis['slo_met']:
            goodput = total_throughput  # P90æ»¡è¶³SLOï¼Œ90%è¯·æ±‚éƒ½æ˜¯è‰¯è¯·æ±‚
            token_goodput = output_token_throughput
            token_goodput_per_user = output_token_throughput_per_user
        else:
            goodput = 0  # P90ä¸æ»¡è¶³SLOï¼Œæ— æ³•ä¿è¯90%çš„è‰¯è¯·æ±‚
            token_goodput = 0
            token_goodput_per_user = 0
        
        return {
            'concurrency': concurrency,
            'slo_config': slo_config,
            'slo': slo,
            'raw_result': result_data,
            'slo_analysis': slo_analysis,
            'total_throughput': total_throughput,
            'request_throughput': goodput,  # è‰¯è¯·æ±‚ååç‡
            'output_token_throughput': output_token_throughput,
            'output_token_throughput_per_user': output_token_throughput_per_user,
            'token_goodput': token_goodput,  # è‰¯tokenååç‡
            'token_goodput_per_user': token_goodput_per_user  # æ¯ç”¨æˆ·è‰¯tokenååç‡
        }
    
    def analyze_slo_satisfaction(self, result: Dict, slo: Dict) -> Dict:
        """åˆ†æSLOæ»¡è¶³ç‡ï¼ˆåŸºäºP90ï¼‰"""
        
        # ä»ç»“æœä¸­æå–P90å»¶è¿Ÿæ•°æ®
        ttft_stats = result.get('time_to_first_token', {})
        itl_stats = result.get('inter_token_latency', {})
        
        # æå–P90å€¼
        ttft_p90 = ttft_stats.get('p90', float('inf'))
        itl_p90 = itl_stats.get('p90', float('inf'))
        
        # åˆ¤æ–­P90æ˜¯å¦æ»¡è¶³SLO
        # P90 < SLO æ„å‘³ç€90%çš„è¯·æ±‚æ»¡è¶³SLO
        ttft_met = ttft_p90 < slo['ttft']
        tpot_met = itl_p90 < slo['tpot']
        
        # ä¸¤ä¸ªæ¡ä»¶éƒ½å¿…é¡»æ»¡è¶³
        slo_met = ttft_met and tpot_met
        
        return {
            'ttft_p90': ttft_p90,
            'itl_p90': itl_p90,
            'ttft_met': ttft_met,
            'tpot_met': tpot_met,
            'slo_met': slo_met,
            'ttft_slo': slo['ttft'],
            'tpot_slo': slo['tpot']
        }
    
    def find_max_goodput(self, slo_config: str) -> int:
        """æ‰¾åˆ°æ»¡è¶³SLOçº¦æŸçš„æœ€å¤§Goodput"""
        
        print(f"\nğŸ” Finding max goodput for {slo_config} SLO...")
        
        # ä»ä½å¹¶å‘å¼€å§‹æµ‹è¯•
        concurrency = 1
        max_goodput = 0
        max_goodput_result = {}
        consecutive_failures = 0
        
        while concurrency <= 1000 and consecutive_failures < 3:
            result = self.run_benchmark_with_slo(concurrency, slo_config)
            
            if result is None:
                consecutive_failures += 1
                concurrency += 10
                continue
                
            slo_analysis = result['slo_analysis']
            
            goodput = result.get('request_throughput', 0)
            total_throughput = result.get('total_throughput', 0)
            token_goodput = result.get('token_goodput', 0)
            token_goodput_per_user = result.get('token_goodput_per_user', 0)
            ttft_p90 = slo_analysis.get('ttft_p90', 0)
            itl_p90 = slo_analysis.get('itl_p90', 0)
            
            if slo_analysis['slo_met']:
                if goodput > max_goodput:
                    max_goodput = goodput
                    max_goodput_result = {
                        'concurrency': concurrency,
                        'request_goodput': goodput,
                        'token_goodput': token_goodput,
                        'token_goodput_per_user': token_goodput_per_user,
                        'ttft_p90': ttft_p90,
                        'itl_p90': itl_p90
                    }
                
                print(f"âœ… Concurrency {concurrency}: SLO satisfied")
                print(f"   P90: TTFT={ttft_p90:.2f}ms, TPOT={itl_p90:.2f}ms")
                print(f"   Request Goodput: {goodput:.2f} req/s")
                print(f"   Token Goodput: {token_goodput:.2f} tokens/s")
                print(f"   Token Goodput/User: {token_goodput_per_user:.2f} tokens/s/user")
                concurrency += 10
                consecutive_failures = 0
            else:
                print(f"âŒ Concurrency {concurrency}: SLO violated (P90: TTFT={ttft_p90:.1f}ms, TPOT={itl_p90:.1f}ms)")
                consecutive_failures += 1
                concurrency += 10
                
                # å¦‚æœè¿ç»­å¤±è´¥ï¼Œåœæ­¢æµ‹è¯•
                if consecutive_failures >= 3:
                    break
        
        print(f"ğŸ¯ Max goodput for {slo_config} SLO:")
        if max_goodput_result:
            print(f"   Concurrency: {max_goodput_result['concurrency']}")
            print(f"   Request Goodput: {max_goodput_result['request_goodput']:.2f} req/s")
            print(f"   Token Goodput: {max_goodput_result['token_goodput']:.2f} tokens/s")
            print(f"   Token Goodput/User: {max_goodput_result['token_goodput_per_user']:.2f} tokens/s/user")
        
        # è¿”å›å®Œæ•´ç»“æœ
        return max_goodput_result if max_goodput_result else {'request_goodput': 0, 'token_goodput': 0, 'token_goodput_per_user': 0}
    

    def analyze_latency_distribution(self, result: Dict) -> Dict:
        """åˆ†æå»¶è¿Ÿåˆ†å¸ƒ"""
        
        ttft_data = result.get('time_to_first_token', {}).get('data', [])
        tpot_data = result.get('inter_token_latency', {}).get('data', [])
        
        if not ttft_data or not tpot_data:
            return {}
        
        analysis = {
            'ttft': {
                'mean': statistics.mean(ttft_data),
                'median': statistics.median(ttft_data),
                'p90': sorted(ttft_data)[int(len(ttft_data) * 0.90)],
                'p95': sorted(ttft_data)[int(len(ttft_data) * 0.95)],
                'p99': sorted(ttft_data)[int(len(ttft_data) * 0.99)],
                'std': statistics.stdev(ttft_data) if len(ttft_data) > 1 else 0
            },
            'tpot': {
                'mean': statistics.mean(tpot_data),
                'median': statistics.median(tpot_data),
                'p90': sorted(tpot_data)[int(len(tpot_data) * 0.90)],
                'p95': sorted(tpot_data)[int(len(tpot_data) * 0.95)],
                'p99': sorted(tpot_data)[int(len(tpot_data) * 0.99)],
                'std': statistics.stdev(tpot_data) if len(tpot_data) > 1 else 0
            }
        }
        
        return analysis
    
    def generate_report(self, results: Dict) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        
        report = []
        report.append("ğŸ“Š DistServe-Style Performance Test Report")
        report.append("=" * 50)
        report.append()
        
        # SLOçº¦æŸæµ‹è¯•ç»“æœ
        report.append("ğŸ¯ SLO Constraint Test Results:")
        report.append("-" * 30)
        
        for config, config_results in results.items():
            report.append(f"\nğŸ“‹ {config}:")
            for slo_config, max_goodput in config_results.items():
                report.append(f"   {slo_config} SLO: {max_goodput} requests/sec")
        
        # æ€§èƒ½å¯¹æ¯”
        report.append("\nğŸ“ˆ Performance Comparison:")
        report.append("-" * 30)
        
        if len(results) >= 2:
            configs = list(results.keys())
            base_config = configs[0]
            
            for slo_config in self.slo_configs.keys():
                base_goodput = results[base_config][slo_config]
                report.append(f"\n{slo_config} SLO:")
                
                for config in configs[1:]:
                    goodput = results[config][slo_config]
                    improvement = (goodput - base_goodput) / base_goodput * 100
                    report.append(f"   {config} vs {base_config}: {improvement:+.2f}%")
        
        # å»ºè®®
        report.append("\nğŸ’¡ Recommendations:")
        report.append("-" * 20)
        report.append("1. ä½¿ç”¨prefix_data_generatorç”ŸæˆçœŸå®æµ‹è¯•æ•°æ®")
        report.append("2. å®æ–½åŠ¨æ€è´Ÿè½½æµ‹è¯•")
        report.append("3. ä¼˜åŒ–èµ„æºé…ç½®")
        report.append("4. ç›‘æ§å»¶è¿Ÿåˆ†å¸ƒ")
        
        return "\n".join(report)
    
    def generate_single_deployment_report(self, deployment_name: str, results: Dict) -> str:
        """ç”Ÿæˆå•ä¸ªéƒ¨ç½²çš„æµ‹è¯•æŠ¥å‘Š"""
        
        report = []
        report.append(f"ğŸ“Š DistServe-Style Performance Test Report - {deployment_name}")
        report.append("=" * 60)
        report.append("")
        
        # SLOçº¦æŸæµ‹è¯•ç»“æœ
        report.append("ğŸ¯ SLO Constraint Test Results:")
        report.append("-" * 30)
        
        for slo_config, result in results.items():
            slo = self.slo_configs[slo_config]
            if isinstance(result, dict):
                req_goodput = result.get('request_goodput', 0)
                token_goodput = result.get('token_goodput', 0)
                token_goodput_per_user = result.get('token_goodput_per_user', 0)
                report.append(f"{slo_config} SLO (TTFT<{slo['ttft']}ms, TPOT<{slo['tpot']}ms):")
                report.append(f"  Request Goodput: {req_goodput:.2f} req/s")
                report.append(f"  Token Goodput: {token_goodput:.2f} tokens/s")
                report.append(f"  Token Goodput/User: {token_goodput_per_user:.2f} tokens/s/user")
            else:
                report.append(f"{slo_config} SLO (TTFT<{slo['ttft']}ms, TPOT<{slo['tpot']}ms): {result} requests/sec")
        
        # æ€§èƒ½åˆ†æ
        report.append("\nğŸ“ˆ Performance Analysis:")
        report.append("-" * 30)
        
        if results:
            # æ‰¾åˆ°æœ€å¤§Request Goodput
            best_slo = None
            max_req_goodput = 0
            for slo_config, result in results.items():
                if isinstance(result, dict):
                    req_goodput = result.get('request_goodput', 0)
                    if req_goodput > max_req_goodput:
                        max_req_goodput = req_goodput
                        best_slo = slo_config
            
            if best_slo:
                best_result = results[best_slo]
                report.append(f"Best Performance ({best_slo} SLO):")
                report.append(f"  Request Goodput: {best_result['request_goodput']:.2f} req/s")
                report.append(f"  Token Goodput: {best_result['token_goodput']:.2f} tokens/s")
                report.append(f"  Token Goodput/User: {best_result['token_goodput_per_user']:.2f} tokens/s/user")
                report.append(f"  Concurrency: {best_result['concurrency']}")
            
            # è®¡ç®—SLOä¸¥æ ¼åº¦å½±å“
            if len(results) > 1:
                strict_result = results.get('strict', {})
                loose_result = results.get('loose', {})
                
                # æå–request_goodputå€¼
                strict_goodput = strict_result.get('request_goodput', 0) if isinstance(strict_result, dict) else strict_result
                loose_goodput = loose_result.get('request_goodput', 0) if isinstance(loose_result, dict) else loose_result
                
                if strict_goodput > 0 and loose_goodput > 0:
                    improvement = (loose_goodput - strict_goodput) / strict_goodput * 100
                    report.append(f"SLO Relaxation Impact: {improvement:+.2f}% (strict â†’ loose)")
        
        # å»ºè®®
        report.append("\nğŸ’¡ Recommendations:")
        report.append("-" * 20)
        report.append("1. å¦‚æœGoodputè¾ƒä½ï¼Œè€ƒè™‘ä¼˜åŒ–èµ„æºé…ç½®")
        report.append("2. å¦‚æœSLOæ»¡è¶³ç‡ä½ï¼Œè€ƒè™‘è°ƒæ•´workeræ¯”ä¾‹")
        report.append("3. ä½¿ç”¨prefix_data_generatorç”ŸæˆçœŸå®æµ‹è¯•æ•°æ®")
        report.append("4. ç›‘æ§å»¶è¿Ÿåˆ†å¸ƒå’Œèµ„æºåˆ©ç”¨ç‡")
        
        return "\n".join(report)
    
    def run_single_deployment_test(self, deployment_name: str = "vllm-agg"):
        """è¿è¡Œå•ä¸ªéƒ¨ç½²çš„DistServeé£æ ¼æµ‹è¯•"""
        
        print(f"ğŸš€ Starting DistServe-style test for {deployment_name}...")
        print("=" * 60)
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        results = {}
        
        # æµ‹è¯•æ‰€æœ‰SLOé…ç½®
        slo_configs = list(self.slo_configs.keys())
        for i, slo_config in enumerate(slo_configs):
            print(f"\nğŸ” [{i+1}/{len(slo_configs)}] Testing {slo_config} SLO...")
            slo = self.slo_configs[slo_config]
            print(f"   SLO: TTFT<{slo['ttft']}ms, TPOT<{slo['tpot']}ms")
            
            try:
                max_goodput = self.find_max_goodput(slo_config)
                results[slo_config] = max_goodput
                print(f"âœ… Completed {slo_config} SLO test")
            except Exception as e:
                print(f"âŒ Error in {slo_config} SLO test: {e}")
                results[slo_config] = 0
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_single_deployment_report(deployment_name, results)
        
        # ä¿å­˜ç»“æœåˆ°benchmarks/resultsç›®å½•
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_dir = "/home/bedicloud/dynamo-main/benchmarks/results"
        os.makedirs(results_dir, exist_ok=True)
        report_file = os.path.join(results_dir, f"distserve_benchmark_{deployment_name}_{timestamp}.txt")
        
        with open(report_file, 'w') as f:
            f.write(report)
        
        print(f"\nğŸ“„ Report saved to: {report_file}")
        print("\n" + report)
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ“š DistServe-Style Performance Test")
    print("Based on: DistServe: Disaggregating Prefill and Decode for Goodput-optimized LLM Serving")
    print("=" * 80)
    
    # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
    service_url = os.environ.get('SERVICE_URL', 'http://127.0.0.1:8005')
    model_name = os.environ.get('DEPLOYMENT_MODEL_ID', 'DeepSeek-R1-Distill-Qwen-7B')
    
    print(f"Service URL: {service_url}")
    print(f"Model Name: {model_name}")
    
    # æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
    try:
        import requests
        health_url = f"{service_url.rstrip('/')}/health"
        response = requests.get(health_url, timeout=5)
        if response.status_code != 200:
            print(f"âŒ Service not available at {service_url}")
            print("Please ensure the service is running and port forwarding is active")
            return
    except Exception as e:
        print(f"âŒ Cannot connect to service at {service_url}: {e}")
        print("Please ensure the service is running and port forwarding is active")
        return
    
    # è¿è¡Œæµ‹è¯•
    tester = DistServeStyleTest(service_url=service_url, model_name=model_name)
    
    # ä»å‘½ä»¤è¡Œå‚æ•°æˆ–ç¯å¢ƒå˜é‡è·å–éƒ¨ç½²åç§°
    import sys
    if len(sys.argv) > 1:
        deployment_name = sys.argv[1]
    else:
        deployment_name = os.environ.get('DEPLOYMENT_NAME', 'vllm-agg')
    
    print(f"Testing deployment: {deployment_name}")
    results = tester.run_single_deployment_test(deployment_name)
    
    print(f"\nğŸ¯ Test completed for {deployment_name}")
    print("Results:", results)

if __name__ == "__main__":
    main()
