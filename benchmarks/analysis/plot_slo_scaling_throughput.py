#!/usr/bin/env python3
"""
æ ¹æ®SLOçº¦æŸæ”¾ç¼©å› å­ç»˜åˆ¶ååç‡æ›²çº¿

åŠŸèƒ½ï¼š
1. å®šä¹‰SLOçº¦æŸå­—å…¸ï¼ˆåŒ…å«å¯æ”¾ç¼©å’Œä¸å¯æ”¾ç¼©çš„æŒ‡æ ‡ï¼‰
2. å¯æ”¾ç¼©æŒ‡æ ‡æŒ‰æ”¾ç¼©å› å­ï¼ˆ5.0åˆ°0.0ï¼Œæ­¥é•¿0.1ï¼‰åŒæ­¥æ”¾ç¼©
3. å¯¹äºæ¯ä¸ªæ”¾ç¼©å› å­ï¼Œæ‰¾åˆ°æ»¡è¶³æ‰€æœ‰çº¦æŸçš„æœ€å¤§ååç‡
4. ç»˜åˆ¶ä¸¤å¼ å›¾ï¼šè¯·æ±‚ååç‡å’Œè¾“å‡ºtokenååç‡
"""

import csv
import argparse
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional


# ==================== SLOçº¦æŸé…ç½® ====================
# åœ¨è¿™é‡Œå®šä¹‰ä½ çš„SLOçº¦æŸå­—å…¸
# scalable_metrics: å¯ä»¥åŒæ­¥æ”¾ç¼©çš„æŒ‡æ ‡
# fixed_metrics: å›ºå®šä¸å˜çš„æŒ‡æ ‡

SLO_CONFIG = {
    'scalable_metrics': {
        # å¯æ”¾ç¼©æŒ‡æ ‡ï¼š{'metric_name': original_value}
        # è¿™äº›æŒ‡æ ‡ä¼šæŒ‰æ”¾ç¼©å› å­åŒæ­¥ç¼©æ”¾
        'time_to_first_token_p90': 4000.0,  # ms
        'inter_token_latency_p90': 10.0,     # ms
    },
    'fixed_metrics': {
        # å›ºå®šæŒ‡æ ‡ï¼š{'metric_name': fixed_value}
        # è¿™äº›æŒ‡æ ‡ä¸å—æ”¾ç¼©å› å­å½±å“
        # å¯ä»¥ç•™ç©º {}ï¼Œè¡¨ç¤ºæ²¡æœ‰å›ºå®šçº¦æŸ
    }
}

# é»˜è®¤åˆ—åæ˜ å°„ï¼ˆCSVä¸­çš„å®é™…åˆ—åï¼‰
METRIC_COLUMN_MAP = {
    'time_to_first_token_p90': 'time_to_first_token_p90',
    'inter_token_latency_p90': 'inter_token_latency_p90',
    'request_throughput_avg': 'request_throughput_avg',
    'output_token_throughput_avg': 'output_token_throughput_avg',
}


def load_csv_data(csv_file: str) -> List[Dict]:
    """ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®"""
    data = []
    with open(csv_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            numeric_row = {}
            for key, value in row.items():
                if key == 'concurrency':
                    numeric_row[key] = int(value) if value else 0
                else:
                    try:
                        numeric_row[key] = float(value) if value else 0.0
                    except (ValueError, TypeError):
                        numeric_row[key] = value
            data.append(numeric_row)
    return data


def check_slo_constraints(
    row: Dict,
    scalable_metrics: Dict[str, float],
    fixed_metrics: Dict[str, float],
    scale_factor: float,
    metric_column_map: Dict[str, str]
) -> bool:
    """
    æ£€æŸ¥æ•°æ®ç‚¹æ˜¯å¦æ»¡è¶³SLOçº¦æŸ
    
    Args:
        row: æ•°æ®è¡Œ
        scalable_metrics: å¯æ”¾ç¼©æŒ‡æ ‡å­—å…¸
        fixed_metrics: å›ºå®šæŒ‡æ ‡å­—å…¸
        scale_factor: æ”¾ç¼©å› å­
        metric_column_map: æŒ‡æ ‡ååˆ°CSVåˆ—åçš„æ˜ å°„
    
    Returns:
        æ˜¯å¦æ»¡è¶³æ‰€æœ‰çº¦æŸ
    """
    # æ£€æŸ¥å¯æ”¾ç¼©æŒ‡æ ‡ï¼ˆå®é™…å€¼ < æ”¾ç¼©åçš„çº¦æŸå€¼ï¼‰
    for metric_name, original_constraint in scalable_metrics.items():
        csv_column = metric_column_map.get(metric_name, metric_name)
        actual_value = row.get(csv_column, float('inf'))
        
        # æ”¾ç¼©åçš„çº¦æŸå€¼ = åŸå§‹å€¼ * æ”¾ç¼©å› å­
        scaled_constraint = original_constraint * scale_factor
        
        # éœ€è¦æ»¡è¶³ï¼šå®é™…å€¼ < æ”¾ç¼©åçš„çº¦æŸå€¼
        if actual_value >= scaled_constraint:
            return False
    
    # æ£€æŸ¥å›ºå®šæŒ‡æ ‡ï¼ˆå®é™…å€¼ < å›ºå®šçº¦æŸå€¼ï¼‰
    for metric_name, fixed_constraint in fixed_metrics.items():
        csv_column = metric_column_map.get(metric_name, metric_name)
        actual_value = row.get(csv_column, float('inf'))
        
        # éœ€è¦æ»¡è¶³ï¼šå®é™…å€¼ < å›ºå®šçº¦æŸå€¼
        if actual_value >= fixed_constraint:
            return False
    
    return True


def find_max_concurrency_for_scale_factor(
    data: List[Dict],
    scalable_metrics: Dict[str, float],
    fixed_metrics: Dict[str, float],
    scale_factor: float,
    metric_column_map: Dict[str, str]
) -> int:
    """
    æ‰¾åˆ°ç»™å®šæ”¾ç¼©å› å­ä¸‹æ»¡è¶³æ‰€æœ‰çº¦æŸçš„æœ€å¤§å¹¶å‘åº¦
    
    Returns:
        max_concurrency: æœ€å¤§å¹¶å‘åº¦ï¼Œå¦‚æœæ²¡æœ‰æ»¡è¶³æ¡ä»¶çš„æ•°æ®åˆ™è¿”å›0
    """
    max_concurrency = 0
    
    for row in data:
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³çº¦æŸ
        if check_slo_constraints(row, scalable_metrics, fixed_metrics, 
                                 scale_factor, metric_column_map):
            concurrency = row.get('concurrency', 0)
            if isinstance(concurrency, (int, float)):
                concurrency = int(concurrency)
                if concurrency > max_concurrency:
                    max_concurrency = concurrency
    
    return max_concurrency


def find_max_throughput_for_scale_factor(
    data: List[Dict],
    scalable_metrics: Dict[str, float],
    fixed_metrics: Dict[str, float],
    scale_factor: float,
    metric_column_map: Dict[str, str],
    request_col: str = 'request_throughput_avg',
    token_col: str = 'output_token_throughput_avg'
) -> Tuple[float, float]:
    """
    æ‰¾åˆ°ç»™å®šæ”¾ç¼©å› å­ä¸‹æ»¡è¶³æ‰€æœ‰çº¦æŸçš„æœ€å¤§ååç‡
    
    Returns:
        (max_request_throughput, max_token_throughput)
    """
    max_request_throughput = 0.0
    max_token_throughput = 0.0
    
    for row in data:
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³çº¦æŸ
        if check_slo_constraints(row, scalable_metrics, fixed_metrics, 
                                 scale_factor, metric_column_map):
            request_throughput = row.get(request_col, 0.0)
            token_throughput = row.get(token_col, 0.0)
            
            if request_throughput > max_request_throughput:
                max_request_throughput = request_throughput
            
            if token_throughput > max_token_throughput:
                max_token_throughput = token_throughput
    
    return max_request_throughput, max_token_throughput


def calculate_scaling_curve_concurrency(
    data: List[Dict],
    scalable_metrics: Dict[str, float],
    fixed_metrics: Dict[str, float],
    metric_column_map: Dict[str, str],
    scale_range: Tuple[float, float] = (0.0, 5.0),
    scale_step: float = 0.1
) -> Tuple[List[float], List[int]]:
    """
    è®¡ç®—æ”¾ç¼©å› å­ä¸æœ€å¤§å¹¶å‘åº¦çš„å…³ç³»æ›²çº¿
    
    Returns:
        (scale_factors, max_concurrencies)
    """
    scale_factors = []
    max_concurrencies = []
    
    scale_min, scale_max = scale_range
    
    # ä»æœ€å¤§å€¼åˆ°æœ€å°å€¼ï¼ˆæˆ–åä¹‹ï¼Œæ ¹æ®éœ€æ±‚ï¼‰
    # è®¡ç®—éœ€è¦å¤šå°‘æ­¥
    num_steps = int((scale_max - scale_min) / scale_step) + 1
    
    for i in range(num_steps):
        current_scale = scale_max - i * scale_step
        # å¤„ç†æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼Œå››èˆäº”å…¥åˆ°åˆé€‚çš„å°æ•°ä½æ•°
        decimal_places = len(str(scale_step).split('.')[-1]) if '.' in str(scale_step) else 0
        current_scale = round(current_scale, decimal_places + 2)
        
        max_concurrency = find_max_concurrency_for_scale_factor(
            data, scalable_metrics, fixed_metrics, current_scale,
            metric_column_map
        )
        
        scale_factors.append(current_scale)
        max_concurrencies.append(max_concurrency)
        
        # å¦‚æœå·²ç»åˆ°è¾¾æœ€å°å€¼ï¼Œé€€å‡º
        if current_scale <= scale_min + 1e-10:
            break
    
    return scale_factors, max_concurrencies


def calculate_scaling_curve_throughput(
    data: List[Dict],
    scalable_metrics: Dict[str, float],
    fixed_metrics: Dict[str, float],
    metric_column_map: Dict[str, str],
    scale_range: Tuple[float, float] = (0.0, 5.0),
    scale_step: float = 0.1,
    request_col: str = 'request_throughput_avg',
    token_col: str = 'output_token_throughput_avg'
) -> Tuple[List[float], List[float], List[float]]:
    """
    è®¡ç®—æ”¾ç¼©å› å­ä¸æœ€å¤§ååç‡çš„å…³ç³»æ›²çº¿
    
    Returns:
        (scale_factors, max_request_throughputs, max_token_throughputs)
    """
    scale_factors = []
    max_request_throughputs = []
    max_token_throughputs = []
    
    scale_min, scale_max = scale_range
    
    # ä»æœ€å¤§å€¼åˆ°æœ€å°å€¼ï¼ˆæˆ–åä¹‹ï¼Œæ ¹æ®éœ€æ±‚ï¼‰
    # è®¡ç®—éœ€è¦å¤šå°‘æ­¥
    num_steps = int((scale_max - scale_min) / scale_step) + 1
    
    for i in range(num_steps):
        current_scale = scale_max - i * scale_step
        # å¤„ç†æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼Œå››èˆäº”å…¥åˆ°åˆé€‚çš„å°æ•°ä½æ•°
        decimal_places = len(str(scale_step).split('.')[-1]) if '.' in str(scale_step) else 0
        current_scale = round(current_scale, decimal_places + 2)
        
        max_req, max_token = find_max_throughput_for_scale_factor(
            data, scalable_metrics, fixed_metrics, current_scale,
            metric_column_map, request_col, token_col
        )
        
        scale_factors.append(current_scale)
        max_request_throughputs.append(max_req)
        max_token_throughputs.append(max_token)
        
        # å¦‚æœå·²ç»åˆ°è¾¾æœ€å°å€¼ï¼Œé€€å‡º
        if current_scale <= scale_min + 1e-10:
            break
    
    return scale_factors, max_request_throughputs, max_token_throughputs


def plot_scaling_concurrency(
    scale_factors: List[float],
    max_concurrencies: List[int],
    output_file: str = None,
    title: str = "Maximum Concurrency vs SLO Scaling Factor",
    isl: float = None,
    osl: float = None,
    scalable_metrics: Dict[str, float] = None,
    fixed_metrics: Dict[str, float] = None,
    label: str = None,
    color: str = 'blue',
    marker: str = 'o'
):
    """
    ç»˜åˆ¶æ”¾ç¼©å› å­ä¸æœ€å¤§å¹¶å‘åº¦çš„å…³ç³»å›¾
    """
    # æ„å»ºæ ‡é¢˜åç¼€
    title_suffix = ""
    if isl is not None and osl is not None:
        title_suffix = f" (ISL={isl:.0f}, OSL={osl:.0f})"
    
    # æ„å»ºçº¦æŸä¿¡æ¯æ–‡æœ¬
    constraint_text = ""
    if scalable_metrics:
        constraint_text += "\nScalable: " + ", ".join([f"{k}={v}" for k, v in scalable_metrics.items()])
    if fixed_metrics:
        constraint_text += "\nFixed: " + ", ".join([f"{k}={v}" for k, v in fixed_metrics.items()])
    
    # åˆ›å»ºå•ä¸ªå›¾
    plt.figure(figsize=(12, 8))
    
    # æ„å»ºæ ‡ç­¾
    plot_label = f"{label} - Max Concurrency" if label else 'Max Concurrency'
    
    # ç»˜åˆ¶æ›²çº¿
    plt.plot(scale_factors, max_concurrencies, marker=marker, linewidth=2, 
             markersize=6, color=color, label=plot_label, alpha=0.8)
    plt.xlabel('SLO Scaling Factor', fontsize=12)
    plt.ylabel('Max Concurrency', fontsize=12)
    plt.title(f'Maximum Concurrency vs SLO Scaling Factor{title_suffix}{constraint_text}', 
              fontsize=14, fontweight='bold')
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, alpha=0.3, linestyle='--')
    # åè½¬xè½´ï¼Œè®©å¤§å€¼åœ¨å·¦è¾¹ï¼Œå°å€¼åœ¨å³è¾¹
    if scale_factors:
        plt.xlim(left=min(scale_factors), right=max(scale_factors))
        plt.gca().invert_xaxis()  # åè½¬xè½´
    if max_concurrencies and max(max_concurrencies) > 0:
        plt.ylim(bottom=0)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤º
    if output_file:
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Plot saved to: {output_file}")
    else:
        plt.show()
    
    plt.close()


def plot_scaling_concurrency_compare(
    scale_factors_agg: List[float],
    max_concurrencies_agg: List[int],
    scale_factors_disagg: List[float],
    max_concurrencies_disagg: List[int],
    output_file: str = None,
    title: str = "Maximum Concurrency vs SLO Scaling Factor",
    isl: float = None,
    osl: float = None,
    scalable_metrics: Dict[str, float] = None,
    fixed_metrics: Dict[str, float] = None
):
    """
    åœ¨åŒä¸€å›¾ä¸­ç»˜åˆ¶aggå’Œdisaggçš„å¯¹æ¯”æ›²çº¿
    """
    # æ„å»ºæ ‡é¢˜åç¼€
    title_suffix = ""
    if isl is not None and osl is not None:
        title_suffix = f" (ISL={isl:.0f}, OSL={osl:.0f})"
    
    # æ„å»ºçº¦æŸä¿¡æ¯æ–‡æœ¬
    constraint_text = ""
    if scalable_metrics:
        constraint_text += "\nScalable: " + ", ".join([f"{k}={v}" for k, v in scalable_metrics.items()])
    if fixed_metrics:
        constraint_text += "\nFixed: " + ", ".join([f"{k}={v}" for k, v in fixed_metrics.items()])
    
    # åˆ›å»ºå•ä¸ªå›¾
    plt.figure(figsize=(12, 8))
    
    # ç»˜åˆ¶aggæ›²çº¿
    plt.plot(scale_factors_agg, max_concurrencies_agg, marker='o', linewidth=2, 
             markersize=6, color='blue', label='Aggregated', alpha=0.8)
    
    # ç»˜åˆ¶disaggæ›²çº¿
    plt.plot(scale_factors_disagg, max_concurrencies_disagg, marker='s', linewidth=2,
             markersize=6, color='red', label='Disaggregated', alpha=0.8)
    
    plt.xlabel('SLO Scaling Factor', fontsize=12)
    plt.ylabel('Max Concurrency', fontsize=12)
    plt.title(f'Maximum Concurrency vs SLO Scaling Factor{title_suffix}{constraint_text} (Agg vs Disagg)', 
              fontsize=14, fontweight='bold')
    plt.legend(loc='best', fontsize=10)
    plt.grid(True, alpha=0.3, linestyle='--')
    
    # åè½¬xè½´ï¼Œè®©å¤§å€¼åœ¨å·¦è¾¹ï¼Œå°å€¼åœ¨å³è¾¹
    all_scale_factors = scale_factors_agg + scale_factors_disagg
    if all_scale_factors:
        plt.xlim(left=min(all_scale_factors), right=max(all_scale_factors))
        plt.gca().invert_xaxis()
    
    all_concurrencies = max_concurrencies_agg + max_concurrencies_disagg
    if all_concurrencies and max(all_concurrencies) > 0:
        plt.ylim(bottom=0)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤º
    if output_file:
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Plot saved to: {output_file}")
    else:
        plt.show()
    
    plt.close()


def plot_scaling_throughput(
    scale_factors: List[float],
    max_request_throughputs: List[float],
    max_token_throughputs: List[float],
    output_file: str = None,
    title: str = "Maximum Throughput vs SLO Scaling Factor",
    isl: float = None,
    osl: float = None,
    scalable_metrics: Dict[str, float] = None,
    fixed_metrics: Dict[str, float] = None,
    label: str = None,
    color_req: str = 'blue',
    color_token: str = 'red',
    marker_req: str = 'o',
    marker_token: str = 's'
):
    """
    ç»˜åˆ¶æ”¾ç¼©å› å­ä¸æœ€å¤§ååç‡çš„å…³ç³»å›¾
    """
    # æ„å»ºæ ‡é¢˜åç¼€
    title_suffix = ""
    if isl is not None and osl is not None:
        title_suffix = f" (ISL={isl:.0f}, OSL={osl:.0f})"
    
    # æ„å»ºçº¦æŸä¿¡æ¯æ–‡æœ¬
    constraint_text = ""
    if scalable_metrics:
        constraint_text += "\nScalable: " + ", ".join([f"{k}={v}" for k, v in scalable_metrics.items()])
    if fixed_metrics:
        constraint_text += "\nFixed: " + ", ".join([f"{k}={v}" for k, v in fixed_metrics.items()])
    
    # åˆ›å»ºä¸¤ä¸ªå­å›¾
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))
    
    # æ„å»ºæ ‡ç­¾
    label_req = f"{label} - Request" if label else 'Max Request Throughput'
    label_token = f"{label} - Token" if label else 'Max Token Throughput'
    
    # ç¬¬ä¸€å¼ å›¾ï¼šè¯·æ±‚ååç‡
    ax1.plot(scale_factors, max_request_throughputs, marker=marker_req, linewidth=2, 
             markersize=6, color=color_req, label=label_req, alpha=0.8)
    ax1.set_xlabel('SLO Scaling Factor', fontsize=12)
    ax1.set_ylabel('Max Request Throughput (req/s)', fontsize=12)
    ax1.set_title(f'Maximum Request Throughput vs SLO Scaling Factor{title_suffix}{constraint_text}', 
                  fontsize=14, fontweight='bold')
    ax1.legend(loc='best', fontsize=10)
    ax1.grid(True, alpha=0.3, linestyle='--')
    if scale_factors:
        ax1.set_xlim(left=min(scale_factors), right=max(scale_factors))
        ax1.invert_xaxis()  # åè½¬xè½´
    if max_request_throughputs and max(max_request_throughputs) > 0:
        ax1.set_ylim(bottom=0)
    
    # ç¬¬äºŒå¼ å›¾ï¼štokenååç‡
    ax2.plot(scale_factors, max_token_throughputs, marker=marker_token, linewidth=2, 
             markersize=6, color=color_token, label=label_token, alpha=0.8)
    ax2.set_xlabel('SLO Scaling Factor', fontsize=12)
    ax2.set_ylabel('Max Token Throughput (tokens/s)', fontsize=12)
    ax2.set_title(f'Maximum Token Throughput vs SLO Scaling Factor{title_suffix}{constraint_text}', 
                  fontsize=14, fontweight='bold')
    ax2.legend(loc='best', fontsize=10)
    ax2.grid(True, alpha=0.3, linestyle='--')
    if scale_factors:
        ax2.set_xlim(left=min(scale_factors), right=max(scale_factors))
        ax2.invert_xaxis()  # åè½¬xè½´
    if max_token_throughputs and max(max_token_throughputs) > 0:
        ax2.set_ylim(bottom=0)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤º
    if output_file:
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Plot saved to: {output_file}")
    else:
        plt.show()
    
    plt.close()


def plot_scaling_throughput_compare(
    scale_factors_agg: List[float],
    max_request_throughputs_agg: List[float],
    max_token_throughputs_agg: List[float],
    scale_factors_disagg: List[float],
    max_request_throughputs_disagg: List[float],
    max_token_throughputs_disagg: List[float],
    output_file: str = None,
    title: str = "Maximum Throughput vs SLO Scaling Factor",
    isl: float = None,
    osl: float = None,
    scalable_metrics: Dict[str, float] = None,
    fixed_metrics: Dict[str, float] = None
):
    """
    åœ¨åŒä¸€å›¾ä¸­ç»˜åˆ¶aggå’Œdisaggçš„å¯¹æ¯”æ›²çº¿
    """
    # æ„å»ºæ ‡é¢˜åç¼€
    title_suffix = ""
    if isl is not None and osl is not None:
        title_suffix = f" (ISL={isl:.0f}, OSL={osl:.0f})"
    
    # æ„å»ºçº¦æŸä¿¡æ¯æ–‡æœ¬
    constraint_text = ""
    if scalable_metrics:
        constraint_text += "\nScalable: " + ", ".join([f"{k}={v}" for k, v in scalable_metrics.items()])
    if fixed_metrics:
        constraint_text += "\nFixed: " + ", ".join([f"{k}={v}" for k, v in fixed_metrics.items()])
    
    # åˆ›å»ºä¸¤ä¸ªå­å›¾
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))
    
    # ç¬¬ä¸€å¼ å›¾ï¼šè¯·æ±‚ååç‡
    ax1.plot(scale_factors_agg, max_request_throughputs_agg, marker='o', linewidth=2, 
             markersize=6, color='blue', label='Aggregated', alpha=0.8)
    ax1.plot(scale_factors_disagg, max_request_throughputs_disagg, marker='s', linewidth=2,
             markersize=6, color='red', label='Disaggregated', alpha=0.8)
    ax1.set_xlabel('SLO Scaling Factor', fontsize=12)
    ax1.set_ylabel('Max Request Throughput (req/s)', fontsize=12)
    ax1.set_title(f'Maximum Request Throughput vs SLO Scaling Factor{title_suffix}{constraint_text} (Agg vs Disagg)', 
                  fontsize=14, fontweight='bold')
    ax1.legend(loc='best', fontsize=10)
    ax1.grid(True, alpha=0.3, linestyle='--')
    all_scale_factors = scale_factors_agg + scale_factors_disagg
    if all_scale_factors:
        ax1.set_xlim(left=min(all_scale_factors), right=max(all_scale_factors))
        ax1.invert_xaxis()  # åè½¬xè½´
    all_req_throughputs = max_request_throughputs_agg + max_request_throughputs_disagg
    if all_req_throughputs and max(all_req_throughputs) > 0:
        ax1.set_ylim(bottom=0)
    
    # ç¬¬äºŒå¼ å›¾ï¼štokenååç‡
    ax2.plot(scale_factors_agg, max_token_throughputs_agg, marker='o', linewidth=2, 
             markersize=6, color='blue', label='Aggregated', alpha=0.8)
    ax2.plot(scale_factors_disagg, max_token_throughputs_disagg, marker='s', linewidth=2,
             markersize=6, color='red', label='Disaggregated', alpha=0.8)
    ax2.set_xlabel('SLO Scaling Factor', fontsize=12)
    ax2.set_ylabel('Max Token Throughput (tokens/s)', fontsize=12)
    ax2.set_title(f'Maximum Token Throughput vs SLO Scaling Factor{title_suffix}{constraint_text} (Agg vs Disagg)', 
                  fontsize=14, fontweight='bold')
    ax2.legend(loc='best', fontsize=10)
    ax2.grid(True, alpha=0.3, linestyle='--')
    if all_scale_factors:
        ax2.set_xlim(left=min(all_scale_factors), right=max(all_scale_factors))
        ax2.invert_xaxis()  # åè½¬xè½´
    all_token_throughputs = max_token_throughputs_agg + max_token_throughputs_disagg
    if all_token_throughputs and max(all_token_throughputs) > 0:
        ax2.set_ylim(bottom=0)
    
    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()
    
    # ä¿å­˜æˆ–æ˜¾ç¤º
    if output_file:
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        print(f"âœ… Plot saved to: {output_file}")
    else:
        plt.show()
    
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description='Plot maximum throughput vs SLO scaling factor'
    )
    parser.add_argument(
        '--csv',
        type=str,
        default=None,
        help='Input CSV file path (single mode)'
    )
    parser.add_argument(
        '--csv-agg',
        type=str,
        default=None,
        help='Aggregated mode CSV file path (for comparison)'
    )
    parser.add_argument(
        '--csv-disagg',
        type=str,
        default=None,
        help='Disaggregated mode CSV file path (for comparison)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='Output image file path'
    )
    parser.add_argument(
        '--scale-min',
        type=float,
        default=0.0,
        help='Minimum scaling factor (default: 0.0)'
    )
    parser.add_argument(
        '--scale-max',
        type=float,
        default=5.0,
        help='Maximum scaling factor (default: 5.0)'
    )
    parser.add_argument(
        '--scale-step',
        type=float,
        default=0.1,
        help='Scaling factor step size (default: 0.1)'
    )
    parser.add_argument(
        '--y-axis',
        type=str,
        choices=['throughput', 'concurrency'],
        default='throughput',
        help='Y-axis metric: throughput (request and token throughput) or concurrency (max concurrency) (default: throughput)'
    )
    
    args = parser.parse_args()
    
    # åˆ¤æ–­æ˜¯å•æ–‡ä»¶æ¨¡å¼è¿˜æ˜¯å¯¹æ¯”æ¨¡å¼
    compare_mode = args.csv_agg is not None and args.csv_disagg is not None
    
    # è·å–SLOé…ç½®
    scalable_metrics = SLO_CONFIG.get('scalable_metrics', {})
    fixed_metrics = SLO_CONFIG.get('fixed_metrics', {})
    
    print(f"\nğŸ“‹ SLO Configuration:")
    if scalable_metrics:
        print(f"   Scalable metrics (will be scaled):")
        for metric, value in scalable_metrics.items():
            print(f"      {metric}: {value}")
    if fixed_metrics:
        print(f"   Fixed metrics (constant):")
        for metric, value in fixed_metrics.items():
            print(f"      {metric}: {value}")
    if not scalable_metrics and not fixed_metrics:
        print("   âš ï¸  No SLO constraints defined! Please edit SLO_CONFIG in the script.")
    
    metric_column_map = METRIC_COLUMN_MAP
    all_metrics = list(scalable_metrics.keys()) + list(fixed_metrics.keys())
    
    if compare_mode:
        # å¯¹æ¯”æ¨¡å¼ï¼šåŠ è½½ä¸¤ä¸ªCSVæ–‡ä»¶
        print(f"ğŸ“Š Loading aggregated data from: {args.csv_agg}")
        data_agg = load_csv_data(args.csv_agg)
        print(f"âœ… Loaded {len(data_agg)} data points (agg)")
        
        print(f"ğŸ“Š Loading disaggregated data from: {args.csv_disagg}")
        data_disagg = load_csv_data(args.csv_disagg)
        print(f"âœ… Loaded {len(data_disagg)} data points (disagg)")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        for data, name in [(data_agg, "agg"), (data_disagg, "disagg")]:
            for metric in all_metrics:
                csv_col = metric_column_map.get(metric, metric)
                if csv_col not in data[0]:
                    print(f"âŒ Error: Column '{csv_col}' (for metric '{metric}') not found in {name} CSV")
                    return
        
        # è¯»å–ISLå’ŒOSL
        isl = data_agg[0].get('input_sequence_length_avg', None) if data_agg else None
        osl = data_agg[0].get('output_sequence_length_avg', None) if data_agg else None
        if isl is None and data_disagg:
            isl = data_disagg[0].get('input_sequence_length_avg', None)
            osl = data_disagg[0].get('output_sequence_length_avg', None)
        
        # è®¡ç®—ä¸¤æ¡æ›²çº¿
        print(f"\nğŸ” Calculating scaling curves...")
        print(f"   Scaling factor range: {args.scale_min} to {args.scale_max}, step: {args.scale_step}")
        print(f"   Y-axis metric: {args.y_axis}")
        
        if args.y_axis == 'concurrency':
            scale_factors_agg, max_concurrencies_agg = calculate_scaling_curve_concurrency(
                data_agg, scalable_metrics, fixed_metrics, metric_column_map,
                scale_range=(args.scale_min, args.scale_max), scale_step=args.scale_step
            )
            
            scale_factors_disagg, max_concurrencies_disagg = calculate_scaling_curve_concurrency(
                data_disagg, scalable_metrics, fixed_metrics, metric_column_map,
                scale_range=(args.scale_min, args.scale_max), scale_step=args.scale_step
            )
            
            print(f"âœ… Calculated {len(scale_factors_agg)} data points for each mode")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if max_concurrencies_agg:
                max_agg = max(max_concurrencies_agg)
                max_agg_idx = max_concurrencies_agg.index(max_agg)
                max_agg_scale = scale_factors_agg[max_agg_idx]
                print(f"\nğŸ“ˆ Aggregated Statistics:")
                print(f"   Maximum concurrency: {max_agg} (at scale factor {max_agg_scale:.2f})")
            
            if max_concurrencies_disagg:
                max_disagg = max(max_concurrencies_disagg)
                max_disagg_idx = max_concurrencies_disagg.index(max_disagg)
                max_disagg_scale = scale_factors_disagg[max_disagg_idx]
                print(f"\nğŸ“ˆ Disaggregated Statistics:")
                print(f"   Maximum concurrency: {max_disagg} (at scale factor {max_disagg_scale:.2f})")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if args.output:
                output_file = args.output
            else:
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"plot_slo_scaling_concurrency_compare_{timestamp}.png"
            
            # ç»˜åˆ¶å¯¹æ¯”å›¾
            print(f"\nğŸ“Š Generating comparison plot...")
            plot_scaling_concurrency_compare(
                scale_factors_agg, max_concurrencies_agg,
                scale_factors_disagg, max_concurrencies_disagg,
                output_file=output_file,
                title="Maximum Concurrency vs SLO Scaling Factor (Agg vs Disagg)",
                isl=isl, osl=osl,
                scalable_metrics=scalable_metrics,
                fixed_metrics=fixed_metrics
            )
        else:
            # throughput æ¨¡å¼
            scale_factors_agg, max_request_throughputs_agg, max_token_throughputs_agg = calculate_scaling_curve_throughput(
                data_agg, scalable_metrics, fixed_metrics, metric_column_map,
                scale_range=(args.scale_min, args.scale_max), scale_step=args.scale_step
            )
            
            scale_factors_disagg, max_request_throughputs_disagg, max_token_throughputs_disagg = calculate_scaling_curve_throughput(
                data_disagg, scalable_metrics, fixed_metrics, metric_column_map,
                scale_range=(args.scale_min, args.scale_max), scale_step=args.scale_step
            )
            
            print(f"âœ… Calculated {len(scale_factors_agg)} data points for each mode")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if args.output:
                output_file = args.output
            else:
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"plot_slo_scaling_throughput_compare_{timestamp}.png"
            
            # ç»˜åˆ¶å¯¹æ¯”å›¾
            print(f"\nğŸ“Š Generating comparison plot...")
            plot_scaling_throughput_compare(
                scale_factors_agg, max_request_throughputs_agg, max_token_throughputs_agg,
                scale_factors_disagg, max_request_throughputs_disagg, max_token_throughputs_disagg,
                output_file=output_file,
                title="Maximum Throughput vs SLO Scaling Factor (Agg vs Disagg)",
                isl=isl, osl=osl,
                scalable_metrics=scalable_metrics,
                fixed_metrics=fixed_metrics
            )
        
    else:
        # å•æ–‡ä»¶æ¨¡å¼
        if not args.csv:
            print("âŒ Error: Either --csv or both --csv-agg and --csv-disagg must be provided")
            return
        
        print(f"ğŸ“Š Loading data from: {args.csv}")
        data = load_csv_data(args.csv)
        print(f"âœ… Loaded {len(data)} data points")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—æ˜¯å¦å­˜åœ¨
        for metric in all_metrics:
            csv_col = metric_column_map.get(metric, metric)
            if csv_col not in data[0]:
                print(f"âŒ Error: Column '{csv_col}' (for metric '{metric}') not found in CSV")
                print(f"   Available columns: {', '.join(list(data[0].keys())[:10])}...")
                return
        
        # è¯»å–ISLå’ŒOSL
        isl = data[0].get('input_sequence_length_avg', None) if data else None
        osl = data[0].get('output_sequence_length_avg', None) if data else None
        
        # è®¡ç®—æ›²çº¿æ•°æ®
        print(f"\nğŸ” Calculating scaling curve...")
        print(f"   Scaling factor range: {args.scale_min} to {args.scale_max}, step: {args.scale_step}")
        print(f"   Y-axis metric: {args.y_axis}")
        
        if args.y_axis == 'concurrency':
            scale_factors, max_concurrencies = calculate_scaling_curve_concurrency(
                data, scalable_metrics, fixed_metrics, metric_column_map,
                scale_range=(args.scale_min, args.scale_max), scale_step=args.scale_step
            )
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"âœ… Calculated {len(scale_factors)} data points")
            
            non_zero = sum(1 for c in max_concurrencies if c > 0)
            print(f"   Non-zero concurrency points: {non_zero}")
            
            # æ‰¾åˆ°æœ€å¤§å€¼
            if max_concurrencies:
                max_concurrency_value = max(max_concurrencies)
                max_concurrency_idx = max_concurrencies.index(max_concurrency_value)
                max_concurrency_scale = scale_factors[max_concurrency_idx]
                print(f"\nğŸ“ˆ Concurrency Statistics:")
                print(f"   Maximum: {max_concurrency_value} (at scale factor {max_concurrency_scale:.2f})")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if args.output:
                output_file = args.output
            else:
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"plot_slo_scaling_concurrency_{timestamp}.png"
            
            # ç»˜åˆ¶å›¾è¡¨
            print(f"\nğŸ“Š Generating plot...")
            plot_scaling_concurrency(
                scale_factors, max_concurrencies,
                output_file=output_file,
                title="Maximum Concurrency vs SLO Scaling Factor",
                isl=isl, osl=osl,
                scalable_metrics=scalable_metrics,
                fixed_metrics=fixed_metrics
            )
        else:
            # throughput æ¨¡å¼
            scale_factors, max_request_throughputs, max_token_throughputs = calculate_scaling_curve_throughput(
                data, scalable_metrics, fixed_metrics, metric_column_map,
                scale_range=(args.scale_min, args.scale_max), scale_step=args.scale_step
            )
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            print(f"âœ… Calculated {len(scale_factors)} data points")
            
            non_zero_req = sum(1 for t in max_request_throughputs if t > 0)
            non_zero_token = sum(1 for t in max_token_throughputs if t > 0)
            print(f"   Non-zero request throughput points: {non_zero_req}")
            print(f"   Non-zero token throughput points: {non_zero_token}")
            
            # æ‰¾åˆ°æœ€å¤§å€¼
            if max_request_throughputs:
                max_req_value = max(max_request_throughputs)
                max_req_idx = max_request_throughputs.index(max_req_value)
                max_req_scale = scale_factors[max_req_idx]
                print(f"\nğŸ“ˆ Request Throughput Statistics:")
                print(f"   Maximum: {max_req_value:.2f} req/s (at scale factor {max_req_scale:.2f})")
            
            if max_token_throughputs:
                max_token_value = max(max_token_throughputs)
                max_token_idx = max_token_throughputs.index(max_token_value)
                max_token_scale = scale_factors[max_token_idx]
                print(f"\nğŸ“ˆ Token Throughput Statistics:")
                print(f"   Maximum: {max_token_value:.2f} tokens/s (at scale factor {max_token_scale:.2f})")
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            if args.output:
                output_file = args.output
            else:
                from datetime import datetime
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = f"plot_slo_scaling_throughput_{timestamp}.png"
            
            # ç»˜åˆ¶å›¾è¡¨
            print(f"\nğŸ“Š Generating plot...")
            plot_scaling_throughput(
                scale_factors, max_request_throughputs, max_token_throughputs,
                output_file=output_file,
                title="Maximum Throughput vs SLO Scaling Factor",
                isl=isl, osl=osl,
                scalable_metrics=scalable_metrics,
                fixed_metrics=fixed_metrics
            )
    
    print(f"\nâœ… Done!")


if __name__ == '__main__':
    main()

